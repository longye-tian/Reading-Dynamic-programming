{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646418ba-ae65-46dc-9096-8cca45243b2f",
   "metadata": {},
   "source": [
    "### EXERCISE 8.1.1.\n",
    "\n",
    "Verify that the monotonicity and consistency conditions hold for Example 8.1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea228f28-84ef-40bd-acbd-de929f31cf8d",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "From (8.6), we have the following value aggregator\n",
    "\n",
    "$$\n",
    "B(x,a,v) = ae(x)+(1-a)\\left[c(x)+\\beta\\sum_{x'}v(x')P(x,x')\\right]\n",
    "$$\n",
    "\n",
    "We first show that this value aggregator is monotonic.\n",
    "\n",
    "Let $v,w\\in V$ and $v\\le w$. Then, we have,\n",
    "\n",
    "$$\n",
    "B(x,a,v) = ae(x)+(1-a)\\left[c(x)+\\beta\\sum_{x'}v(x')P(x,x')\\right]\\le ae(x)+(1-a)\\left[c(x)+\\beta\\sum_{x'}w(x')P(x,x')\\right] = B(x,a,w)\n",
    "$$\n",
    "\n",
    "Then, we prove that this value aggregator is consistent\n",
    "\n",
    "Let $\\sigma\\in \\Sigma$ and $v\\in V$, we have,\n",
    "\n",
    "$$\n",
    "B(x,\\sigma(x),v) = \\sigma(x)e(x)+(1-\\sigma(x))\\left[c(x)+\\beta\\sum_{x'}v(x')P(x,x')\\right]\n",
    "$$\n",
    "\n",
    "Since $\\sigma(x)\\in \\Gamma(x)$, this implies, $(x,\\sigma(x))\\in G$, hence, $w(x)=B(x,\\sigma(x),v)\\in\\mathbb{R}$. This gives us the consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11295d-ff39-46eb-a43d-97b62202eaf3",
   "metadata": {},
   "source": [
    "### EXERCISE 8.1.2.\n",
    "\n",
    "Show that the RDP in Example 8.1.4. can also be expressed as an MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd46108-0d9a-46c1-b1f4-aa35356a338f",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "We let\n",
    "\n",
    "- $X:=Y\\times Z$ denote the state space, where\n",
    "- $Y$: denote the action space\n",
    "- $Z$: denote the exogenous space, with $(Z_t)$ is $Q$-Markov\n",
    "\n",
    "We have the feasible correspondence as\n",
    "\n",
    "$$\n",
    "\\Gamma: X\\mapsto \\mathcal{P}(A)\n",
    "$$\n",
    "\n",
    "Let \n",
    "\n",
    "- $\\beta$: denote the discount factor\n",
    "- $F: G\\times A\\mapsto \\mathbb{R}$ denote the current reward\n",
    "- $P((y,z), q, (y',z')) = \\mathbb{1}\\{y'=q\\}Q(z,z')$ be the stochastic kernel.\n",
    "\n",
    "Now, we set $\\mathcal{M} = (\\Gamma, \\beta, F, P)$ is the MDP representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746df3d2-0f2c-4bdf-9ee8-f1436ce2158f",
   "metadata": {},
   "source": [
    "### EXERCISE 8.1.3.\n",
    "\n",
    "Verify that $(\\Gamma, V,B)$ as defined in Example 8.1.5. is an RDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df868f-bd84-43ac-a272-f15695e4ddb9",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "We need to show that $B$ satisfies the monotonicity and consistency condition.\n",
    "\n",
    "We have,\n",
    "\n",
    "$$\n",
    "B(x,a,v) = r(x,a)+\\sum_{x'}v(x')\\beta(x,a,x')P(x,a,x')\n",
    "$$\n",
    "\n",
    "For monotonicity, let $v,w\\in V$ and $v\\le w$, we have,\n",
    "\n",
    "$$\n",
    "B(x,a,v) = r(x,a)+\\sum_{x'}v(x')\\beta(x,a,x')P(x,a,x')\\le r(x,a)+\\sum_{x'}w(x')\\beta(x,a,x')P(x,a,x') = B(x,a,w)\n",
    "$$\n",
    "\n",
    "For consistency, let $\\sigma\\in\\Sigma$, $v\\in V$, we have,\n",
    "\n",
    "$$\n",
    "B(x,\\sigma(x),v) = r(x,\\sigma(x))+\\sum_{x'}v(x')\\beta(x,\\sigma(x),x')P(x,\\sigma(x),x')\n",
    "$$\n",
    "\n",
    "Since we have $\\sigma(x)\\in \\Gamma(x)$, we have $w(x)=B(x,\\sigma(x),v)\\in\\mathbb{R}$. This proves consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6954e4c-2719-493b-971c-21a90ddad435",
   "metadata": {},
   "source": [
    "### EXERCISE 8.1.4.\n",
    "\n",
    "Confirm that the risk-sensitive model $(\\Gamma, V,B)$ in Example 8.1.6. is an RDP for all nonzero $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea033c-4118-4860-9779-36769ebecaa1",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "We have,\n",
    "\n",
    "$$\n",
    "B(x,a,v) = r(x,a)+\\beta\\frac{1}{\\theta}\\ln\\left\\{\\sum_{x'}\\exp(\\theta v(x'))P(x,a,x')\\right\\}\n",
    "$$\n",
    "\n",
    "We need to show monotonicity and consistency holds.\n",
    "\n",
    "For monotonicity, let $v,w\\in V$ and $v\\le w$. Then we have,\n",
    "\n",
    "$$\n",
    "B(x,a,v) = r(x,a)+\\beta\\frac{1}{\\theta}\\ln\\left\\{\\sum_{x'}\\exp(\\theta v(x'))P(x,a,x')\\right\\}\\le r(x,a)+\\beta\\frac{1}{\\theta}\\ln\\left\\{\\sum_{x'}\\exp(\\theta w(x'))P(x,a,x')\\right\\} = B(x,a,w)\n",
    "$$\n",
    "\n",
    "For consistency, let $\\sigma\\in\\Sigma$, $v\\in V$, we have,\n",
    "\n",
    "$$\n",
    "B(x,\\sigma(x),v) = r(x,\\sigma(x))+\\beta\\frac{1}{\\theta}\\ln\\left\\{\\sum_{x'}\\exp(\\theta v(x'))P(x,\\sigma(x),x')\\right\\}\n",
    "$$\n",
    "\n",
    "Since we have $\\sigma(x)\\in \\Gamma(x)$, we have $w(x)=B(x,\\sigma(x),v)\\in\\mathbb{R}$. This proves consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6092716-891e-4610-9edc-2f546cb4ce6b",
   "metadata": {},
   "source": [
    "### EXERCISE 8.1.5.\n",
    "\n",
    "Let $V$ be all strictly positive functions in $\\mathbb{R}^X$. Then $(\\Gamma, V,B)$ in Example 8.1.7. is an RDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb06a9-95cb-47ce-8654-a68cc863ca31",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "We have,\n",
    "\n",
    "$$\n",
    "B(x,a,v) = \\left\\{r(x,a)+\\beta\\left[\\sum_{x'}v(x')^\\gamma P(x,a,x')\\right]^{\\alpha/\\gamma}\\right\\}^{1/\\alpha}\n",
    "$$\n",
    "\n",
    "We want to show that monotonicity and consistency holds.\n",
    "\n",
    "For monotonicity, let $v,w\\in V$ and $0\\ll v\\le w$, we have,\n",
    "\n",
    "$$\n",
    "B(x,a,v) = \\left\\{r(x,a)+\\beta\\left[\\sum_{x'}v(x')^\\gamma P(x,a,x')\\right]^{\\alpha/\\gamma}\\right\\}^{1/\\alpha}\\le \\left\\{r(x,a)+\\beta\\left[\\sum_{x'}w(x')^\\gamma P(x,a,x')\\right]^{\\alpha/\\gamma}\\right\\}^{1/\\alpha} = B(x,a,w)\n",
    "$$\n",
    "\n",
    "\n",
    "For consistency, let $\\sigma\\in\\Sigma$, $v\\in V$, we have,\n",
    "\n",
    "$$\n",
    "B(x,\\sigma(x),v) = \\left\\{r(x,\\sigma(x))+\\beta\\left[\\sum_{x'}v(x')^\\gamma P(x,\\sigma(x),x')\\right]^{\\alpha/\\gamma}\\right\\}^{1/\\alpha}\n",
    "$$\n",
    "\n",
    "Since we have $\\sigma(x)\\in \\Gamma(x)$, we have $w(x)=B(x,\\sigma(x),v)\\in\\mathbb{R}$. This proves consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482ce76-8da2-46c4-9155-6073caf4bd7d",
   "metadata": {},
   "source": [
    "### EXERCISE 8.1.6.\n",
    "\n",
    "Show that $T_\\sigma$ is an order-preserving self-map on $V$ for all $\\sigma\\in\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b3a27-719b-4925-b00f-93022cad5c6b",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "Since $(T_\\sigma v)(x) = B(x,\\sigma(x),v)$ and by consistency, we have $T_\\sigma v\\in V$. Hence, $T_\\sigma$ is a self-map.\n",
    "\n",
    "Let $v,w\\in V$ and $v\\le w$.\n",
    "\n",
    "Then we have,\n",
    "\n",
    "$$\n",
    "(T_\\sigma v)(x) = B(x,\\sigma(x),v)\\le B(x,\\sigma(x),w) = (T_\\sigma w)(x)\n",
    "$$\n",
    "\n",
    "By the monotonicity of $B$.\n",
    "\n",
    "Hence $T_\\sigma$ is order-preserving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52011e7a-1160-4be6-9b51-4da5ed4386ff",
   "metadata": {},
   "source": [
    "### EXERCISE 8.1.7.\n",
    "\n",
    "Show that for each $v\\in V$, the set $\\{T_\\sigma v\\}_{\\sigma\\in\\Sigma}\\subset V$ contains a least and greatest element.\n",
    "\n",
    "Explain the connection between the greatest element and $v$-greedy policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27283c23-1bc9-4bda-aa9c-4ffe24c9dcd0",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "By definition, we have,\n",
    "\n",
    "$$\n",
    "(T_\\sigma v)(x_i) = B(x,\\sigma(x_i),v)\n",
    "$$\n",
    "\n",
    "for $i=1,2,\\cdots,N$, where $N$ is the cardinality of $X$.\n",
    "\n",
    "Let $\\sigma_i\\in \\arg\\max_{\\sigma\\in\\Sigma} B(x,\\sigma(x),v)$.\n",
    "\n",
    "Let $\\sigma (x_i)=\\sigma_i(x_i)$, then $\\sigma (x_i)\\ge \\sigma'(x_i)$ for all $\\sigma'\\in\\Sigma$. Hence, we have a greatest element. This is the v-greedy policy.\n",
    "\n",
    "Similarly, change the above argument to minimum, we also get a least element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f05ad-0d7d-4a47-bceb-4692107b3bba",
   "metadata": {},
   "source": [
    "### EXERCISE 8.1.8.\n",
    "\n",
    "Given RDP $\\mathcal{R}=(\\Gamma, V, B)$ with policy operators $\\{T_\\sigma\\}$ and Bellman operator $T$,  show that, for each $v\\in V$,\n",
    "\n",
    "1. $Tv= \\bigvee_\\sigma T_\\sigma v:= \\bigvee_{\\sigma\\in\\Sigma}(T_\\sigma v)$\n",
    "2. $\\sigma$ is $v$-greedy if and only if $Tv=T_\\sigma v$\n",
    "3. $T$ is an order-preserving self-map on $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31622181-11b2-4657-b50f-e28b5441f16b",
   "metadata": {},
   "source": [
    "**Proof part 1**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
