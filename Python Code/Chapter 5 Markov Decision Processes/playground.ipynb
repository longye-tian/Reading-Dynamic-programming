{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e2b7b4-c3c7-4d3e-a65c-c624fd3b6433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0 with error 45.791431513942214.\n",
      "Terminated successfully in 13 interations.\n",
      "Execution time: 1.513730764389038 seconds\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#              DYNAMIC PROGRAMMING BY JOHN STACHURSKI AND THOMAS SARGENT                     #\n",
    "#                                                                                            #\n",
    "# This code is used for Chapter 5 Markov DECISION PROCESSES:                                 #\n",
    "# Application: OPTIMAL SAVING WITH LABOR INCOME                                              #\n",
    "# Improved computation efficiency using numba.njit                                           #\n",
    "# Written by Longye Tian 02/07/2024                                                          #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                               IMPORT LIBRARIES AND PACKAGES                                #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit                                       \n",
    "import time\n",
    "from scipy.sparse.linalg import bicgstab\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------#\n",
    "#                        USE NAMEDTUPLE TO STORE MODEL PARAMETERS                             #\n",
    "# --------------------------------------------------------------------------------------------#\n",
    "\n",
    "Optimal_Saving_MDP = namedtuple(\"saving_mdp\", \n",
    "                                (\"R\",                          # gross interest rate\n",
    "                                 \"β\",                          # discount rate\n",
    "                                 \"γ\",                          # CRRA\n",
    "                                 \"W\",                          # Wealth state space\n",
    "                                 \"w_size\",\n",
    "                                 \"ρ\",                          # Tauchen\n",
    "                                 \"ν\",                          # Tauchen\n",
    "                                 \"m\",                          # Tauchen\n",
    "                                 \"y_size\"                      # income cardinality\n",
    "                                ))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                       CREATE A FUNCTION TO INPUT MODEL PARAMETERS                           #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def create_saving_mdp_model(R=1.01, \n",
    "                            β=0.98, \n",
    "                            γ=2.5, \n",
    "                            w_min=0.01, \n",
    "                            w_max=20.0, \n",
    "                            w_size=200, \n",
    "                            ρ=0.9, \n",
    "                            ν=0.1, \n",
    "                            m=3,\n",
    "                            y_size=5):\n",
    "    W = np.linspace(w_min, w_max, w_size)\n",
    "    return Optimal_Saving_MDP(R=R, \n",
    "                              β=β, \n",
    "                              γ=γ, \n",
    "                              W=W,\n",
    "                              w_size=w_size,\n",
    "                              ρ=ρ, \n",
    "                              ν=ν, \n",
    "                              m=m,\n",
    "                              y_size=y_size)\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                      NORMAL CDF                                             #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def norm_cdf(x, mean=0, std=1):\n",
    "    # Transform x to the standard normal\n",
    "    z = (x - mean) / std\n",
    "    \n",
    "    # Use the Abramowitz & Stegun approximation for standard normal\n",
    "    t = 1 / (1 + 0.2316419 * np.abs(z))\n",
    "    d = 0.3989423 * np.exp(-z * z / 2)\n",
    "    p = d * t * (0.3193815 + t * (-0.3565638 + t * (1.781478 + t * (-1.821256 + t * 1.330274))))\n",
    "    \n",
    "    return 1 - p if z > 0 else p\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                TAUCHEN DISCRETIZATION                                       #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def Tauchen(saving_mdp):  \n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    σ_y = np.sqrt(ν**2/(1-ρ**2))                               # W's std\n",
    "    Y = np.linspace(-m*σ_y, m*σ_y, y_size)                     # State space by Tauchen\n",
    "    s = (Y[y_size-1]-Y[0])/(y_size-1)                          # gap between two states\n",
    "    Q = np.zeros((y_size,y_size))                              # Initialize Q\n",
    "    for i in range(y_size):\n",
    "        Q[i,0] = norm_cdf(Y[0]-ρ*Y[i]+s/2, std=σ_y)            \n",
    "        Q[i,y_size-1] = 1 - norm_cdf(Y[y_size-1]-ρ*Y[i]-s/2, std=σ_y)   \n",
    "        for j in range(1,y_size-1):\n",
    "            Q[i,j] = norm_cdf(Y[j]-ρ*Y[i]+s/2, std=σ_y)-norm_cdf(Y[j]-ρ*Y[i]-s/2, std=σ_y)\n",
    "    Y = np.exp(Y)\n",
    "    return Y,Q\n",
    "\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# Y,Q = Tauchen(saving_mdp)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                   Utility Function                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def u(c, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    return ((c**(1-γ)))/(1-γ)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                                   BELLMAN EQUATION FOR V                                   #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "@njit\n",
    "def B(v, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    \n",
    "    W = np.reshape(W, (w_size, 1, 1))\n",
    "    Y = np.reshape(Y, (1, y_size, 1))\n",
    "    WP = np.reshape(W, (1, 1, w_size))\n",
    "    \n",
    "    v = np.reshape(v, (1, 1, w_size, y_size))\n",
    "    Q = np.reshape(Q, (1, y_size, 1, y_size))\n",
    "    \n",
    "    c = W+Y-(WP/R)\n",
    "    EV = np.sum(v * Q, axis=-1)\n",
    "    \n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# v = np.zeros((200,5))\n",
    "# B(v, saving_mdp)\n",
    "#σ = get_greedy(v, saving_mdp)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                     Greedy Policy                                           #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def get_greedy(v, saving_mdp):\n",
    "    return np.argmax(B(v, saving_mdp), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                   BELLMAN OPERATOR                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def T(v, saving_mdp):\n",
    "    new_B = B(v, saving_mdp)\n",
    "    w_size = new_B.shape[2]\n",
    "    new_v = np.empty(new_B.shape[:2])\n",
    "    for i in range(new_B.shape[0]):\n",
    "        for j in range(new_B.shape[1]):\n",
    "            new_v[i, j] = np.max(new_B[i, j, :])\n",
    "    return new_v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# v = np.zeros((200,5))\n",
    "# T(v, saving_mdp)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                SUCCESSIVE APPROXIMATION                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def successive_approx (T,                                        # A callable operator\n",
    "                       v_init,                                   # Initial condition\n",
    "                       saving_mdp,                               # Model parameter\n",
    "                       tol = 1e-6,                               # Error tolerance\n",
    "                       max_iter = 10_000,                        # max iterations\n",
    "                       print_step = 25                           # Print at multiples of print_step\n",
    "                      ):\n",
    "    v = v_init                                                   # set the initial condition\n",
    "    error = tol + 1                                              # Initialize the error\n",
    "    k = 0                                                        # initialize the iteration\n",
    "    \n",
    "    while error > tol and k < max_iter: \n",
    "        new_v = T(v,saving_mdp)                                  # update by applying operator T\n",
    "        error = np.max(np.abs(new_v-v))                          # update the error\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\") \n",
    "        v = new_v                                                # update x\n",
    "        k += 1                                                   # update the steps\n",
    "    if error <= tol:                                    \n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "    else:     \n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v\n",
    "\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                                   POLICY OPERATOR                                          #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "# for loop to be compatible with numba\n",
    "@njit\n",
    "def T_σ(v,σ,saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    Σ = np.empty_like(σ)\n",
    "    for i in range(w_size):\n",
    "        for j in range(y_size):\n",
    "           Σ[i,j] = W[σ[i,j]]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "\n",
    "    EV = np.empty((w_size, y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            EV[i,j] = np.sum(np.array([v[σ[i,j],k] *  Q[j,k] for k in np.arange(y_size)]))\n",
    "\n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "\n",
    "# multi-dimensional indexing without numba\n",
    "\n",
    "def T_σ_vec(v,σ,saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "\n",
    "    V = v[σ]\n",
    "    Q = np.reshape(Q, (1,y_size,y_size))\n",
    "    EV = np.sum(V * Q, axis=-1)\n",
    "    \n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                           COMPUTE REWARD AND REWARD OPERATOR  --- HPI                       #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def compute_r_σ(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    r_σ = np.where(c>0, u(c, saving_mdp), -np.inf)\n",
    "    return r_σ\n",
    "\n",
    "\n",
    "def R_σ(v, σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    σ = np.reshape(σ, (w_size, y_size,1))\n",
    "    V = v[σ]\n",
    "    Q = np.reshape(Q, (1,y_size,y_size))\n",
    "    EV = np.sum(V * Q, axis=-1)\n",
    "    return v - β * EV\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                  POLICY EVALUATION --HPI                                    #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# use bicgstab to get value\n",
    "\n",
    "def get_value_bicgstab(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    r_σ = compute_r_σ(σ, saving_mdp)\n",
    "    def _R_σ(v):\n",
    "        return R_σ(v, σ, saving_mdp)\n",
    "    \n",
    "    A = LinearOperator((w_size * y_size, w_size * y_size), matvec=_R_σ)\n",
    "    return bicgstab(A, r_σ)[0]\n",
    "\n",
    "\n",
    "# use normal way to get value\n",
    "def get_value(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    \n",
    "    Σ = np.zeros((w_size,y_size))\n",
    "    for i in range(w_size):\n",
    "       for j in range(y_size):\n",
    "           Σ[i,j] = W[σ[i,j]]\n",
    " \n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    r_σ = np.where(c>0, u(c, saving_mdp), -np.inf)\n",
    "    x_size = w_size * y_size\n",
    "    P_σ = np.zeros((w_size,y_size,w_size,y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            for k in np.arange(y_size):\n",
    "                P_σ[i,j,σ[i,j],k] = Q[j,k]\n",
    "\n",
    "    r_σ = np.reshape(r_σ, (x_size,1))\n",
    "    P_σ = np.reshape(P_σ, (x_size,x_size))\n",
    "    I = np.eye(x_size)\n",
    "    v_σ = np.linalg.solve((I-β*P_σ), r_σ)\n",
    "    \n",
    "    return np.reshape(v_σ, (w_size,y_size))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                      ALGORITHMS                                             #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                VALUE FUNCTION ITERATION                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def value_function_iteration(saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v_init = np.zeros((w_size, y_size), dtype=np.float64)\n",
    "    v_star = successive_approx(T, v_init, saving_mdp)\n",
    "    σ_star = get_greedy(v_star, saving_mdp)\n",
    "    return v_star,  σ_star\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                              OPTIMISTIC POLICY ITERATION                                    #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Currently OPI does not converge if we use T_σ (for loops indexing with numba)\n",
    "\n",
    "# OPI converges if we use T_σ_vec (multi-dimensional indexing without numba)\n",
    "\n",
    "def optimistic_policy_iteration(saving_mdp,\n",
    "                                M=100,\n",
    "                                tol=1e-6, \n",
    "                                max_iter=10_000,\n",
    "                                print_step=25):\n",
    "    \n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v = np.zeros((w_size, y_size))\n",
    "    error = tol+1\n",
    "    k = 0 \n",
    "\n",
    "    while error > tol and k < max_iter:\n",
    "        last_v = v\n",
    "        σ = get_greedy(last_v,saving_mdp)\n",
    "        for i in range(M):\n",
    "            v = T_σ_vec(v, σ, saving_mdp)\n",
    "        error = np.max(np.abs(last_v-v))\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        k += 1\n",
    "    if error <= tol:                                    \n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "        v_star_opi = v\n",
    "        σ_star_opi = get_greedy(v_star_opi, saving_mdp)\n",
    "    else:     \n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v_star_opi, σ_star_opi\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                HOWARD POLICY ITERATIONS                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "def howard_policy_iteration(saving_mdp, \n",
    "                            tol=1e-6, \n",
    "                            max_iter=10_000, \n",
    "                            print_step=25):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v = np.zeros((w_size, y_size))\n",
    "    error = 1 + tol\n",
    "    k=0\n",
    "    while error > tol and k < max_iter:\n",
    "        σ = get_greedy(v, saving_mdp)\n",
    "        v_σ = get_value(σ, saving_mdp)\n",
    "        error = np.max(np.abs(v_σ-v))\n",
    "        v = v_σ\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        k += 1\n",
    "    if error <= tol:\n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "        v_star_hpi = v\n",
    "        σ_star_hpi = get_greedy(v_star_hpi, saving_mdp)\n",
    "    else:\n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v_star_hpi, σ_star_hpi\n",
    "        \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                         PLAYGROUND                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#-------------ON YOUR MARKS---------SET----------------BANG!----------------------------------#\n",
    "\n",
    "\n",
    "saving_mdp = create_saving_mdp_model()\n",
    "#v_star_opi, σ_star_opi = optimistic_policy_iteration(saving_mdp)\n",
    "\n",
    "v_star_hpi, σ_star_hpi = howard_policy_iteration(saving_mdp)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------#\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fe16d35-22d8-4d46-bda1-9619cd9318d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_value(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y, Q = Tauchen(saving_mdp)\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    \n",
    "    # Use a large negative number instead of -np.inf\n",
    "    r_σ = np.where(c > 0, u(c, saving_mdp), -np.inf)\n",
    "    \n",
    "    x_size = w_size * y_size\n",
    "    P_σ = np.zeros((w_size,y_size,w_size,y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            for k in np.arange(y_size):\n",
    "                P_σ[i,j,σ[i,j],k] = Q[j,k]\n",
    "    P_σ = np.reshape(P_σ, (x_size,x_size))\n",
    "\n",
    "    r_σ = np.reshape(r_σ, (x_size,1))\n",
    "    I = np.eye(x_size)\n",
    "    \n",
    "    # Use a more stable solving method\n",
    "    v_σ = np.linalg.solve(I - β*P_σ, r_σ)\n",
    "    return np.reshape(v_σ, (w_size, y_size))\n",
    "\n",
    "def howard_policy_iteration(saving_mdp, \n",
    "                            tol=1e-6, \n",
    "                            max_iter=10_000, \n",
    "                            print_step=25):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v = np.zeros((w_size, y_size))\n",
    "    error = tol + 1\n",
    "    k = 0\n",
    "    while error > tol and k < max_iter:\n",
    "        σ = get_greedy(v, saving_mdp)\n",
    "        v_new = get_value(σ, saving_mdp)\n",
    "        \n",
    "        # Use relative error for better convergence criterion\n",
    "        error = np.max(np.abs(v_new - v))\n",
    "        \n",
    "        v = v_new\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        k += 1\n",
    "    if error <= tol:\n",
    "        print(f\"Terminated successfully in {k} iterations.\")\n",
    "        v_star_hpi = v\n",
    "        σ_star_hpi = get_greedy(v_star_hpi, saving_mdp)\n",
    "    else:\n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v_star_hpi, σ_star_hpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2366375-5d46-4cdb-b6b2-458471a63584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0 with error 45.791431513942214.\n",
      "Terminated successfully in 13 interations.\n",
      "Execution time: 0.34987616539001465 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#-------------ON YOUR MARKS---------SET----------------BANG!----------------------------------#\n",
    "\n",
    "\n",
    "saving_mdp = create_saving_mdp_model()\n",
    "#v_star_opi, σ_star_opi = optimistic_policy_iteration(saving_mdp)\n",
    "\n",
    "v_star_hpi, σ_star_hpi = howard_policy_iteration(saving_mdp)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------#\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a082e-1cae-4598-94c0-c7e0616c6793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((33,33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e132e4fe-0880-44c5-b105-2669f4d759ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0 with error 45.791431513942214.\n",
      "Terminated successfully in 13 interations.\n",
      "Execution time: 1.589123010635376 seconds\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#              DYNAMIC PROGRAMMING BY JOHN STACHURSKI AND THOMAS SARGENT                     #\n",
    "#                                                                                            #\n",
    "# This code is used for Chapter 5 Markov DECISION PROCESSES:                                 #\n",
    "# Application: OPTIMAL SAVING WITH LABOR INCOME                                              #\n",
    "# Improved computation efficiency using numba.njit                                           #\n",
    "# Written by Longye Tian 02/07/2024                                                          #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                               IMPORT LIBRARIES AND PACKAGES                                #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit                                       \n",
    "import time\n",
    "from scipy.sparse.linalg import bicgstab\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------#\n",
    "#                        USE NAMEDTUPLE TO STORE MODEL PARAMETERS                             #\n",
    "# --------------------------------------------------------------------------------------------#\n",
    "\n",
    "Optimal_Saving_MDP = namedtuple(\"saving_mdp\", \n",
    "                                (\"R\",                          # gross interest rate\n",
    "                                 \"β\",                          # discount rate\n",
    "                                 \"γ\",                          # CRRA\n",
    "                                 \"W\",                          # Wealth state space\n",
    "                                 \"w_size\",\n",
    "                                 \"ρ\",                          # Tauchen\n",
    "                                 \"ν\",                          # Tauchen\n",
    "                                 \"m\",                          # Tauchen\n",
    "                                 \"y_size\"                      # income cardinality\n",
    "                                ))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                       CREATE A FUNCTION TO INPUT MODEL PARAMETERS                           #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def create_saving_mdp_model(R=1.01, \n",
    "                            β=0.98, \n",
    "                            γ=2.5, \n",
    "                            w_min=0.01, \n",
    "                            w_max=20.0, \n",
    "                            w_size=200, \n",
    "                            ρ=0.9, \n",
    "                            ν=0.1, \n",
    "                            m=3,\n",
    "                            y_size=5):\n",
    "    W = np.linspace(w_min, w_max, w_size)\n",
    "    return Optimal_Saving_MDP(R=R, \n",
    "                              β=β, \n",
    "                              γ=γ, \n",
    "                              W=W,\n",
    "                              w_size=w_size,\n",
    "                              ρ=ρ, \n",
    "                              ν=ν, \n",
    "                              m=m,\n",
    "                              y_size=y_size)\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                      NORMAL CDF                                             #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def norm_cdf(x, mean=0, std=1):\n",
    "    # Transform x to the standard normal\n",
    "    z = (x - mean) / std\n",
    "    \n",
    "    # Use the Abramowitz & Stegun approximation for standard normal\n",
    "    t = 1 / (1 + 0.2316419 * np.abs(z))\n",
    "    d = 0.3989423 * np.exp(-z * z / 2)\n",
    "    p = d * t * (0.3193815 + t * (-0.3565638 + t * (1.781478 + t * (-1.821256 + t * 1.330274))))\n",
    "    \n",
    "    return 1 - p if z > 0 else p\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                TAUCHEN DISCRETIZATION                                       #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def Tauchen(saving_mdp):  \n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    σ_y = np.sqrt(ν**2/(1-ρ**2))                               # W's std\n",
    "    Y = np.linspace(-m*σ_y, m*σ_y, y_size)                     # State space by Tauchen\n",
    "    s = (Y[y_size-1]-Y[0])/(y_size-1)                          # gap between two states\n",
    "    Q = np.zeros((y_size,y_size))                              # Initialize Q\n",
    "    for i in range(y_size):\n",
    "        Q[i,0] = norm_cdf(Y[0]-ρ*Y[i]+s/2, std=σ_y)            \n",
    "        Q[i,y_size-1] = 1 - norm_cdf(Y[y_size-1]-ρ*Y[i]-s/2, std=σ_y)   \n",
    "        for j in range(1,y_size-1):\n",
    "            Q[i,j] = norm_cdf(Y[j]-ρ*Y[i]+s/2, std=σ_y)-norm_cdf(Y[j]-ρ*Y[i]-s/2, std=σ_y)\n",
    "    Y = np.exp(Y)\n",
    "    return Y,Q\n",
    "\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# Y,Q = Tauchen(saving_mdp)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                   Utility Function                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def u(c, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    return ((c**(1-γ)))/(1-γ)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                                   BELLMAN EQUATION FOR V                                   #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "@njit\n",
    "def B(v, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    \n",
    "    W = np.reshape(W, (w_size, 1, 1))\n",
    "    Y = np.reshape(Y, (1, y_size, 1))\n",
    "    WP = np.reshape(W, (1, 1, w_size))\n",
    "    \n",
    "    v = np.reshape(v, (1, 1, w_size, y_size))\n",
    "    Q = np.reshape(Q, (1, y_size, 1, y_size))\n",
    "    \n",
    "    c = W+Y-(WP/R)\n",
    "    EV = np.sum(v * Q, axis=-1)\n",
    "    \n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# v = np.zeros((200,5))\n",
    "# B(v, saving_mdp)\n",
    "#σ = get_greedy(v, saving_mdp)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                     Greedy Policy                                           #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def get_greedy(v, saving_mdp):\n",
    "    return np.argmax(B(v, saving_mdp), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                   BELLMAN OPERATOR                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def T(v, saving_mdp):\n",
    "    new_B = B(v, saving_mdp)\n",
    "    w_size = new_B.shape[2]\n",
    "    new_v = np.empty(new_B.shape[:2])\n",
    "    for i in range(new_B.shape[0]):\n",
    "        for j in range(new_B.shape[1]):\n",
    "            new_v[i, j] = np.max(new_B[i, j, :])\n",
    "    return new_v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# v = np.zeros((200,5))\n",
    "# T(v, saving_mdp)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                SUCCESSIVE APPROXIMATION                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def successive_approx (T,                                        # A callable operator\n",
    "                       v_init,                                   # Initial condition\n",
    "                       saving_mdp,                               # Model parameter\n",
    "                       tol = 1e-6,                               # Error tolerance\n",
    "                       max_iter = 10_000,                        # max iterations\n",
    "                       print_step = 25                           # Print at multiples of print_step\n",
    "                      ):\n",
    "    v = v_init                                                   # set the initial condition\n",
    "    error = tol + 1                                              # Initialize the error\n",
    "    k = 0                                                        # initialize the iteration\n",
    "    \n",
    "    while error > tol and k < max_iter: \n",
    "        new_v = T(v,saving_mdp)                                  # update by applying operator T\n",
    "        error = np.max(np.abs(new_v-v))                          # update the error\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\") \n",
    "        v = new_v                                                # update x\n",
    "        k += 1                                                   # update the steps\n",
    "    if error <= tol:                                    \n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "    else:     \n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v\n",
    "\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                                   POLICY OPERATOR                                          #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "# for loop to be compatible with numba\n",
    "# The problem is in np.empty_like \n",
    "# If we initialize Σ = np.zeros((w_size, y_size))\n",
    "@njit\n",
    "def T_σ(v,σ,saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    \n",
    "    # Σ = np.empty_like(σ)\n",
    "    Σ = np.zeros((w_size,y_size))\n",
    "    \n",
    "    for i in range(w_size):\n",
    "        for j in range(y_size):\n",
    "           Σ[i,j] = W[σ[i,j]]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "\n",
    "    EV = np.empty((w_size, y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            EV[i,j] = np.sum(np.array([v[σ[i,j],k] *  Q[j,k] for k in np.arange(y_size)]))\n",
    "\n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "\n",
    "# multi-dimensional indexing without numba works\n",
    "\n",
    "def T_σ_vec(v,σ,saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "\n",
    "    V = v[σ]\n",
    "    Q = np.reshape(Q, (1,y_size,y_size))\n",
    "    EV = np.sum(V * Q, axis=-1)\n",
    "    \n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                           COMPUTE REWARD AND REWARD OPERATOR  --- HPI                       #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def compute_r_σ(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    r_σ = np.where(c>0, u(c, saving_mdp), -np.inf)\n",
    "    return r_σ\n",
    "\n",
    "\n",
    "def R_σ(v, σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    σ = np.reshape(σ, (w_size, y_size,1))\n",
    "    V = v[σ]\n",
    "    Q = np.reshape(Q, (1,y_size,y_size))\n",
    "    EV = np.sum(V * Q, axis=-1)\n",
    "    return v - β * EV\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                  POLICY EVALUATION --HPI                                    #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# use bicgstab to get value\n",
    "\n",
    "def get_value_bicgstab(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    r_σ = compute_r_σ(σ, saving_mdp)\n",
    "    def _R_σ(v):\n",
    "        return R_σ(v, σ, saving_mdp)\n",
    "    \n",
    "    A = LinearOperator((w_size * y_size, w_size * y_size), matvec=_R_σ)\n",
    "    return bicgstab(A, r_σ)[0]\n",
    "\n",
    "\n",
    "# use normal way to get value\n",
    "\n",
    "def get_value(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    r_σ = np.where(c>0, u(c, saving_mdp), -np.inf)\n",
    "    x_size = w_size * y_size\n",
    "    P_σ = np.zeros((w_size,y_size,w_size,y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            for k in np.arange(y_size):\n",
    "                P_σ[i,j,σ[i,j],k] = Q[j,k]\n",
    "\n",
    "    r_σ = np.reshape(r_σ, (x_size,1))\n",
    "    P_σ = np.reshape(P_σ, (x_size,x_size))\n",
    "    I = np.eye(x_size)\n",
    "    v_σ = np.linalg.solve((I-β*P_σ), r_σ)\n",
    "    v_σ = np.reshape(v_σ, (w_size, y_size))\n",
    "    \n",
    "    return v_σ\n",
    "\n",
    "\n",
    "# not working without using multi-indexing \n",
    "# solved, this is due to np.empty_like, avoiding using this function, just use np.zeros\n",
    "\n",
    "def get_value_not_working(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "\n",
    "    # not working part\n",
    "    #Σ = np.empty_like(σ)\n",
    "    #for i in range(w_size):\n",
    "    #   for j in range(y_size):\n",
    "    #      Σ[i,j] = W[σ[i,j]]\n",
    "\n",
    "    # Solved: using np.zeros, will work\n",
    "    Σ = np.zeros((w_size,y_size))\n",
    "    for i in range(w_size):\n",
    "      for j in range(y_size):\n",
    "          Σ[i,j] = W[σ[i,j]]\n",
    "    \n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    r_σ = np.where(c>0, u(c, saving_mdp), -np.inf)\n",
    "    x_size = w_size * y_size\n",
    "    P_σ = np.zeros((w_size,y_size,w_size,y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            for k in np.arange(y_size):\n",
    "                P_σ[i,j,σ[i,j],k] = Q[j,k]\n",
    "\n",
    "    r_σ = np.reshape(r_σ, (x_size,1))\n",
    "    P_σ = np.reshape(P_σ, (x_size,x_size))\n",
    "    I = np.eye(x_size)\n",
    "    v_σ = np.linalg.solve((I-β*P_σ), r_σ)\n",
    "    \n",
    "    return np.reshape(v_σ, (w_size,y_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                      ALGORITHMS                                             #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                VALUE FUNCTION ITERATION                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def value_function_iteration(saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v_init = np.zeros((w_size, y_size), dtype=np.float64)\n",
    "    v_star = successive_approx(T, v_init, saving_mdp)\n",
    "    σ_star = get_greedy(v_star, saving_mdp)\n",
    "    return v_star,  σ_star\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                              OPTIMISTIC POLICY ITERATION                                    #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Currently OPI does not converge if we use T_σ (for loops indexing with numba)\n",
    "\n",
    "# OPI converges if we use T_σ_vec (multi-dimensional indexing without numba)\n",
    "\n",
    "def optimistic_policy_iteration(saving_mdp,\n",
    "                                M=100,\n",
    "                                tol=1e-6, \n",
    "                                max_iter=10_000,\n",
    "                                print_step=25):\n",
    "    \n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v = np.zeros((w_size, y_size))\n",
    "    error = tol+1\n",
    "    k = 0 \n",
    "\n",
    "    while error > tol and k < max_iter:\n",
    "        last_v = v\n",
    "        σ = get_greedy(last_v,saving_mdp)\n",
    "        for i in range(M):\n",
    "            v = T_σ_vec(v, σ, saving_mdp)\n",
    "        error = np.max(np.abs(last_v-v))\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        k += 1\n",
    "    if error <= tol:                                    \n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "        v_star_opi = v\n",
    "        σ_star_opi = get_greedy(v_star_opi, saving_mdp)\n",
    "    else:     \n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v_star_opi, σ_star_opi\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                HOWARD POLICY ITERATIONS                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def howard_policy_iteration(saving_mdp, \n",
    "                            tol=1e-6, \n",
    "                            max_iter=10_000, \n",
    "                            print_step=25):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v = np.zeros((w_size, y_size))\n",
    "    error = 1 + tol\n",
    "    k=0\n",
    "    while error > tol and k < max_iter:\n",
    "        σ = get_greedy(v, saving_mdp)\n",
    "        v_σ = get_value_not_working(σ, saving_mdp)\n",
    "        error = np.max(np.abs(v_σ-v))\n",
    "        v = v_σ\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        k += 1\n",
    "    if error <= tol:\n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "        v_star_hpi = v\n",
    "        σ_star_hpi = get_greedy(v_star_hpi, saving_mdp)\n",
    "    else:\n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v_star_hpi, σ_star_hpi\n",
    "        \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                         PLAYGROUND                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#-------------ON YOUR MARKS---------SET----------------BANG!----------------------------------#\n",
    "\n",
    "\n",
    "saving_mdp = create_saving_mdp_model()\n",
    "#v_star_opi, σ_star_opi = optimistic_policy_iteration(saving_mdp)\n",
    "\n",
    "v_star_hpi, σ_star_hpi = howard_policy_iteration(saving_mdp)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------#\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ed1a96b-a3da-45b9-ab72-4348967befd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0 with error 45.791431513942214.\n",
      "Terminated successfully in 13 interations.\n",
      "Execution time: 1.6308209896087646 seconds\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#              DYNAMIC PROGRAMMING BY JOHN STACHURSKI AND THOMAS SARGENT                     #\n",
    "#                                                                                            #\n",
    "# This code is used for Chapter 5 Markov DECISION PROCESSES:                                 #\n",
    "# Application: OPTIMAL SAVING WITH LABOR INCOME                                              #\n",
    "# Improved computation efficiency using numba.njit                                           #\n",
    "# Written by Longye Tian 02/07/2024                                                          #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                               IMPORT LIBRARIES AND PACKAGES                                #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit                                       \n",
    "import time\n",
    "from scipy.sparse.linalg import bicgstab\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------#\n",
    "#                        USE NAMEDTUPLE TO STORE MODEL PARAMETERS                             #\n",
    "# --------------------------------------------------------------------------------------------#\n",
    "\n",
    "Optimal_Saving_MDP = namedtuple(\"saving_mdp\", \n",
    "                                (\"R\",                          # gross interest rate\n",
    "                                 \"β\",                          # discount rate\n",
    "                                 \"γ\",                          # CRRA\n",
    "                                 \"W\",                          # Wealth state space\n",
    "                                 \"w_size\",\n",
    "                                 \"ρ\",                          # Tauchen\n",
    "                                 \"ν\",                          # Tauchen\n",
    "                                 \"m\",                          # Tauchen\n",
    "                                 \"y_size\"                      # income cardinality\n",
    "                                ))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                       CREATE A FUNCTION TO INPUT MODEL PARAMETERS                           #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def create_saving_mdp_model(R=1.01, \n",
    "                            β=0.98, \n",
    "                            γ=2.5, \n",
    "                            w_min=0.01, \n",
    "                            w_max=20.0, \n",
    "                            w_size=200, \n",
    "                            ρ=0.9, \n",
    "                            ν=0.1, \n",
    "                            m=3,\n",
    "                            y_size=5):\n",
    "    W = np.linspace(w_min, w_max, w_size)\n",
    "    return Optimal_Saving_MDP(R=R, \n",
    "                              β=β, \n",
    "                              γ=γ, \n",
    "                              W=W,\n",
    "                              w_size=w_size,\n",
    "                              ρ=ρ, \n",
    "                              ν=ν, \n",
    "                              m=m,\n",
    "                              y_size=y_size)\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                      NORMAL CDF                                             #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def norm_cdf(x, mean=0, std=1):\n",
    "    # Transform x to the standard normal\n",
    "    z = (x - mean) / std\n",
    "    \n",
    "    # Use the Abramowitz & Stegun approximation for standard normal\n",
    "    t = 1 / (1 + 0.2316419 * np.abs(z))\n",
    "    d = 0.3989423 * np.exp(-z * z / 2)\n",
    "    p = d * t * (0.3193815 + t * (-0.3565638 + t * (1.781478 + t * (-1.821256 + t * 1.330274))))\n",
    "    \n",
    "    return 1 - p if z > 0 else p\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                TAUCHEN DISCRETIZATION                                       #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def Tauchen(saving_mdp):  \n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    σ_y = np.sqrt(ν**2/(1-ρ**2))                               # W's std\n",
    "    Y = np.linspace(-m*σ_y, m*σ_y, y_size)                     # State space by Tauchen\n",
    "    s = (Y[y_size-1]-Y[0])/(y_size-1)                          # gap between two states\n",
    "    Q = np.zeros((y_size,y_size))                              # Initialize Q\n",
    "    for i in range(y_size):\n",
    "        Q[i,0] = norm_cdf(Y[0]-ρ*Y[i]+s/2, std=σ_y)            \n",
    "        Q[i,y_size-1] = 1 - norm_cdf(Y[y_size-1]-ρ*Y[i]-s/2, std=σ_y)   \n",
    "        for j in range(1,y_size-1):\n",
    "            Q[i,j] = norm_cdf(Y[j]-ρ*Y[i]+s/2, std=σ_y)-norm_cdf(Y[j]-ρ*Y[i]-s/2, std=σ_y)\n",
    "    Y = np.exp(Y)\n",
    "    return Y,Q\n",
    "\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# Y,Q = Tauchen(saving_mdp)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                   Utility Function                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def u(c, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    return ((c**(1-γ)))/(1-γ)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                                   BELLMAN EQUATION FOR V                                   #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "@njit\n",
    "def B(v, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    \n",
    "    W = np.reshape(W, (w_size, 1, 1))\n",
    "    Y = np.reshape(Y, (1, y_size, 1))\n",
    "    WP = np.reshape(W, (1, 1, w_size))\n",
    "    \n",
    "    v = np.reshape(v, (1, 1, w_size, y_size))\n",
    "    Q = np.reshape(Q, (1, y_size, 1, y_size))\n",
    "    \n",
    "    c = W+Y-(WP/R)\n",
    "    EV = np.sum(v * Q, axis=-1)\n",
    "    \n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# v = np.zeros((200,5))\n",
    "# B(v, saving_mdp)\n",
    "#σ = get_greedy(v, saving_mdp)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                     Greedy Policy                                           #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def get_greedy(v, saving_mdp):\n",
    "    return np.argmax(B(v, saving_mdp), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                   BELLMAN OPERATOR                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def T(v, saving_mdp):\n",
    "    new_B = B(v, saving_mdp)\n",
    "    w_size = new_B.shape[2]\n",
    "    new_v = np.empty(new_B.shape[:2])\n",
    "    for i in range(new_B.shape[0]):\n",
    "        for j in range(new_B.shape[1]):\n",
    "            new_v[i, j] = np.max(new_B[i, j, :])\n",
    "    return new_v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# saving_mdp = create_saving_mdp_model()\n",
    "# v = np.zeros((200,5))\n",
    "# T(v, saving_mdp)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                SUCCESSIVE APPROXIMATION                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "@njit\n",
    "def successive_approx (T,                                        # A callable operator\n",
    "                       v_init,                                   # Initial condition\n",
    "                       saving_mdp,                               # Model parameter\n",
    "                       tol = 1e-6,                               # Error tolerance\n",
    "                       max_iter = 10_000,                        # max iterations\n",
    "                       print_step = 25                           # Print at multiples of print_step\n",
    "                      ):\n",
    "    v = v_init                                                   # set the initial condition\n",
    "    error = tol + 1                                              # Initialize the error\n",
    "    k = 0                                                        # initialize the iteration\n",
    "    \n",
    "    while error > tol and k < max_iter: \n",
    "        new_v = T(v,saving_mdp)                                  # update by applying operator T\n",
    "        error = np.max(np.abs(new_v-v))                          # update the error\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\") \n",
    "        v = new_v                                                # update x\n",
    "        k += 1                                                   # update the steps\n",
    "    if error <= tol:                                    \n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "    else:     \n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v\n",
    "\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------#\n",
    "#                                   POLICY OPERATOR                                          #\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "# for loop to be compatible with numba\n",
    "# The problem is in np.empty_like \n",
    "# If we initialize Σ = np.zeros((w_size, y_size))\n",
    "@njit\n",
    "def T_σ(v,σ,saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    \n",
    "    # Σ = np.empty_like(σ)\n",
    "    Σ = np.zeros((w_size,y_size))\n",
    "    \n",
    "    for i in range(w_size):\n",
    "        for j in range(y_size):\n",
    "           Σ[i,j] = W[σ[i,j]]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "\n",
    "    EV = np.empty((w_size, y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            EV[i,j] = np.sum(np.array([v[σ[i,j],k] *  Q[j,k] for k in np.arange(y_size)]))\n",
    "\n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "\n",
    "# multi-dimensional indexing without numba works\n",
    "\n",
    "def T_σ_vec(v,σ,saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "\n",
    "    V = v[σ]\n",
    "    Q = np.reshape(Q, (1,y_size,y_size))\n",
    "    EV = np.sum(V * Q, axis=-1)\n",
    "    \n",
    "    return np.where(c>0, u(c, saving_mdp) + β * EV, -np.inf)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                           COMPUTE REWARD AND REWARD OPERATOR  --- HPI                       #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def compute_r_σ(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    r_σ = np.where(c>0, u(c, saving_mdp), -np.inf)\n",
    "    return r_σ\n",
    "\n",
    "\n",
    "def R_σ(v, σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    σ = np.reshape(σ, (w_size, y_size,1))\n",
    "    V = v[σ]\n",
    "    Q = np.reshape(Q, (1,y_size,y_size))\n",
    "    EV = np.sum(V * Q, axis=-1)\n",
    "    return v - β * EV\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                  POLICY EVALUATION --HPI                                    #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# use bicgstab to get value\n",
    "\n",
    "def get_value_bicgstab(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    r_σ = compute_r_σ(σ, saving_mdp)\n",
    "    def _R_σ(v):\n",
    "        return R_σ(v, σ, saving_mdp)\n",
    "    \n",
    "    A = LinearOperator((w_size * y_size, w_size * y_size), matvec=_R_σ)\n",
    "    return bicgstab(A, r_σ)[0]\n",
    "\n",
    "\n",
    "# use normal way to get value\n",
    "\n",
    "def get_value(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "    Σ = W[σ]\n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    r_σ = np.where(c>0, u(c, saving_mdp), -np.inf)\n",
    "    x_size = w_size * y_size\n",
    "    P_σ = np.zeros((w_size,y_size,w_size,y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            for k in np.arange(y_size):\n",
    "                P_σ[i,j,σ[i,j],k] = Q[j,k]\n",
    "\n",
    "    r_σ = np.reshape(r_σ, (x_size,1))\n",
    "    P_σ = np.reshape(P_σ, (x_size,x_size))\n",
    "    I = np.eye(x_size)\n",
    "    v_σ = np.linalg.solve((I-β*P_σ), r_σ)\n",
    "    v_σ = np.reshape(v_σ, (w_size, y_size))\n",
    "    \n",
    "    return v_σ\n",
    "\n",
    "\n",
    "# not working without using multi-indexing \n",
    "# solved, this is due to np.empty_like, avoiding using this function, just use np.zeros\n",
    "@njit\n",
    "def get_value_not_working_later_resolved(σ, saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    Y,Q = Tauchen(saving_mdp)\n",
    "\n",
    "    # not working part\n",
    "    #Σ = np.empty_like(σ)\n",
    "    #for i in range(w_size):\n",
    "    #   for j in range(y_size):\n",
    "    #      Σ[i,j] = W[σ[i,j]]\n",
    "\n",
    "    # Solved: using np.zeros, will work\n",
    "    Σ = np.zeros((w_size,y_size))\n",
    "    for i in range(w_size):\n",
    "      for j in range(y_size):\n",
    "          Σ[i,j] = W[σ[i,j]]\n",
    "    \n",
    "    W = np.reshape(W, (w_size, 1))\n",
    "    Y = np.reshape(Y, (1, y_size))\n",
    "    c = W + Y - (Σ/R)\n",
    "    r_σ = np.where(c>0, u(c, saving_mdp), -np.inf)\n",
    "    x_size = w_size * y_size\n",
    "    P_σ = np.zeros((w_size,y_size,w_size,y_size))\n",
    "    for i in np.arange(w_size):\n",
    "        for j in np.arange(y_size):\n",
    "            for k in np.arange(y_size):\n",
    "                P_σ[i,j,σ[i,j],k] = Q[j,k]\n",
    "\n",
    "    r_σ = np.reshape(r_σ, (x_size,1))\n",
    "    P_σ = np.reshape(P_σ, (x_size,x_size))\n",
    "    I = np.eye(x_size)\n",
    "    v_σ = np.linalg.solve((I-β*P_σ), r_σ)\n",
    "    \n",
    "    return np.reshape(v_σ, (w_size,y_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                      ALGORITHMS                                             #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                VALUE FUNCTION ITERATION                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "def value_function_iteration(saving_mdp):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v_init = np.zeros((w_size, y_size), dtype=np.float64)\n",
    "    v_star = successive_approx(T, v_init, saving_mdp)\n",
    "    σ_star = get_greedy(v_star, saving_mdp)\n",
    "    return v_star,  σ_star\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                              OPTIMISTIC POLICY ITERATION                                    #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Currently OPI does not converge if we use T_σ (for loops indexing with numba)\n",
    "\n",
    "# OPI converges if we use T_σ_vec (multi-dimensional indexing without numba)\n",
    "\n",
    "def optimistic_policy_iteration(saving_mdp,\n",
    "                                M=100,\n",
    "                                tol=1e-6, \n",
    "                                max_iter=10_000,\n",
    "                                print_step=25):\n",
    "    \n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v = np.zeros((w_size, y_size))\n",
    "    error = tol+1\n",
    "    k = 0 \n",
    "\n",
    "    while error > tol and k < max_iter:\n",
    "        last_v = v\n",
    "        σ = get_greedy(last_v,saving_mdp)\n",
    "        for i in range(M):\n",
    "            v = T_σ_vec(v, σ, saving_mdp)\n",
    "        error = np.max(np.abs(last_v-v))\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        k += 1\n",
    "    if error <= tol:                                    \n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "        v_star_opi = v\n",
    "        σ_star_opi = get_greedy(v_star_opi, saving_mdp)\n",
    "    else:     \n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v_star_opi, σ_star_opi\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                HOWARD POLICY ITERATIONS                                     #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "def howard_policy_iteration(saving_mdp, \n",
    "                            tol=1e-6, \n",
    "                            max_iter=10_000, \n",
    "                            print_step=25):\n",
    "    R, β, γ, W, w_size, ρ, ν, m, y_size = saving_mdp\n",
    "    v = np.zeros((w_size, y_size))\n",
    "    error = 1 + tol\n",
    "    k=0\n",
    "    while error > tol and k < max_iter:\n",
    "        σ = get_greedy(v, saving_mdp)\n",
    "        v_σ = get_value(σ, saving_mdp)\n",
    "        error = np.max(np.abs(v_σ-v))\n",
    "        v = v_σ\n",
    "        if k % print_step == 0:                                   \n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        k += 1\n",
    "    if error <= tol:\n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "        v_star_hpi = v\n",
    "        σ_star_hpi = get_greedy(v_star_hpi, saving_mdp)\n",
    "    else:\n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return v_star_hpi, σ_star_hpi\n",
    "        \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "#                                         PLAYGROUND                                          #\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#-------------ON YOUR MARKS---------SET----------------BANG!----------------------------------#\n",
    "\n",
    "\n",
    "saving_mdp = create_saving_mdp_model()\n",
    "#v_star_opi, σ_star_opi = optimistic_policy_iteration(saving_mdp)\n",
    "\n",
    "v_star_hpi, σ_star_hpi = howard_policy_iteration(saving_mdp)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------#\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5bb55e8-abfd-4991-9402-875f5562b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0 with error 45.791431513942214.\n",
      "Terminated successfully in 13 interations.\n",
      "Execution time: 0.34313488006591797 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#-------------ON YOUR MARKS---------SET----------------BANG!----------------------------------#\n",
    "\n",
    "\n",
    "saving_mdp = create_saving_mdp_model()\n",
    "#v_star_opi, σ_star_opi = optimistic_policy_iteration(saving_mdp)\n",
    "\n",
    "v_star_hpi, σ_star_hpi = howard_policy_iteration(saving_mdp)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------#\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cda9679-8023-47e5-9934-609d9555964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "206da371-a210-4804-bb82-d31a5eabda45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "maximum supported dimension for an ndarray is 32, found 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: maximum supported dimension for an ndarray is 32, found 100"
     ]
    }
   ],
   "source": [
    "\n",
    "A = np.empty(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "420b13ac-0c94-4f4e-a201-395a271059d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ..., -inf, -inf, -inf],\n",
       "       [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "       [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "       ...,\n",
       "       [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "       [-inf, -inf, -inf, ..., -inf, -inf, -inf],\n",
       "       [-inf, -inf, -inf, ..., -inf, -inf, -inf]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6808ae-fb13-4085-8e70-76a191d9249c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
