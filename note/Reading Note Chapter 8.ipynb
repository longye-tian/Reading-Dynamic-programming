{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8705be-3a0f-4307-bac0-a52f7f36f891",
   "metadata": {},
   "source": [
    "# Definitions and Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f1f6a-16d2-40a1-ac38-b9ccd52fc0a3",
   "metadata": {},
   "source": [
    "## Defining RDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1373c35-3bd2-45bd-9cab-e1a6f199a1d9",
   "metadata": {},
   "source": [
    "Consider a generic Bellman equation\n",
    "\n",
    "$$\n",
    "v(x) = \\max_{a\\in\\Gamma{x}}B(x,a,v)\n",
    "$$\n",
    "\n",
    "where, \n",
    "\n",
    "- $x$ is state\n",
    "- $a$ is action\n",
    "- $\\Gamma$ is a feasible correspondence\n",
    "- $B$ is an aggregator function\n",
    "\n",
    "\n",
    "**Recursive Decision Process**\n",
    "\n",
    "We define a **Recursive Decision Process (RDP)** to be a triple $\\mathcal{R}=(\\Gamma, V, B)$ consisting of\n",
    "\n",
    "- **Feasible correspondence** $\\Gamma$ that is a nonempty correpondence from $X$ to $A$, which in turns defines\n",
    "\n",
    "   - **Feasible state-action pair** $G=\\{(x,a)\\in X\\times A: a\\in \\Gamma(x)\\}$\n",
    "   - **Feasible policy set** $\\Sigma=\\{\\sigma \\in A^X: \\sigma(x)\\in \\Gamma(x)\\}$\n",
    "\n",
    "- **Value space** A subset $V\\subset\\mathbb{R}^X$\n",
    "- **Value aggregator** $B: G\\times V\\to \\mathbb{R}$, it is the total lifetime reward corresponding to current action $a$, current state $x$ and value function $v$, that satisfies\n",
    "   - **Monotonicity**: $v,w\\in V, v\\le w\\implies B(x,a,v)\\le B(x,a,w)$\n",
    "   - **Consistency**: $w(x) = B(x,\\sigma(x),w)$ for some $\\sigma\\in\\Sigma$ and $v\\in V\\implies w\\in V$.\n",
    " \n",
    "(The monotonicity condition states relative to $v$, if rewards are at least as high for $w$ in every future state, then the total rewards one can extract under $w$ should be at least as high.)\n",
    "\n",
    "(The consistency condition ensures that as we consider values of different policies we remain within the value space $V$).\n",
    "\n",
    "We can treat MDP as a special case of RDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1763aab-8e36-4018-8fd5-d8d961061513",
   "metadata": {},
   "source": [
    "## Lifetime Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b156e-747e-4a05-96e6-8829c7eafb8b",
   "metadata": {},
   "source": [
    "### Policy and Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3145d-3ad7-47c5-a133-18dfde4cff79",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R}=(\\Gamma, V,B)$ be an RDP with state and action space $X$ and $A$, let $\\Sigma$ be the set of all feasible policies. For each $\\sigma\\in\\Sigma$, we introduce the **policy operator** $T_\\sigma$ as a self-map on $V$ defined by\n",
    "\n",
    "$$\n",
    "(T_\\sigma v)(x) = B(x,\\sigma(x),x)\n",
    "$$\n",
    "\n",
    "and $T_\\sigma$ is an order-preserving self-map on $V$.\n",
    "\n",
    "If $T_\\sigma$ has a unique fixed point in $V$, we denote this fixed point by $v_\\sigma$ and call it the $\\sigma$-value function.\n",
    "\n",
    "We can interpret $v_\\sigma$ as representing the lifetime value of following policy $\\sigma$.\n",
    "\n",
    "\n",
    "**IN RDP**\n",
    "\n",
    "The policy operator can be expressed as $(T_\\sigma v)(x) = A_\\sigma(x,(R_\\sigma v)(x))$ for some aggregator $A_\\sigma$ and certainty equivalent operator $R_\\sigma$. \n",
    "\n",
    "Hence $T_\\sigma$ is a Koopmans operator and lifetime value associated with policy $\\sigma$ is the fixed point of this operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36860eaf-261f-47ae-a3fa-4e8eb35c6238",
   "metadata": {},
   "source": [
    "### Uniqueness and Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc9e19-ba39-46d4-bef0-a5f040a9f06c",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R} = (\\Gamma, V, B)$ be a given RDP with policy operators $\\{T_\\sigma\\}$\n",
    "\n",
    "Given that our objective is to maximize lifetime value over the set of policies in $\\Sigma$, we need to assume at the very least that lifetime value is well defined at each policy.\n",
    "\n",
    "**Well-Posed**\n",
    "\n",
    "We say that $\\mathcal{R}$ is **well-posed** if for all $\\sigma\\in\\Sigma$, $T_\\sigma$ has a unique fixed point $v_\\sigma\\in V$.\n",
    "\n",
    "**Globally stable**\n",
    "\n",
    "Let $\\mathcal{R}$ be an RDP with policy operators $\\{T_\\sigma\\}_{\\sigma\\in\\Sigma}$.\n",
    "\n",
    "We say $\\mathcal{R}$ is **globally stable** if for all $\\sigma\\in\\Sigma$, $T_\\sigma$ is globally stable on $V$.\n",
    "\n",
    "**Every gloablly stable RDP is well-posed.**\n",
    "\n",
    "**Global stability implies that for any choice of terminal condition, finite horizon valuations always converge to their infinite horizon counterparts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa9da5-fce1-4bb8-ac89-e4cad3431571",
   "metadata": {},
   "source": [
    "### Continuity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2e3b9-9a44-42d2-bcec-bc0d30743be5",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R}=(\\Gamma, V,B)$ be an RDP. \n",
    "\n",
    "We call $\\mathcal{R}$ **continuous** if $B(x,a,v)$ is continuous in $v$ for all $(x,a)\\in G$. \n",
    "\n",
    "In other words, $\\mathcal{R}$ is continuous if for any $v\\in V$, any $(x,a)\\in G$ and any sequence $(v_k)_{k\\ge 1}$ in $V$, we have,\n",
    "\n",
    "$$\n",
    "\\lim_{k\\to\\infty} v_k = v \\implies \\lim_{k\\to\\infty}B(x,a,v_k) = B(x,a,v) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656ed06-b351-4cec-a405-ae2b668b54df",
   "metadata": {},
   "source": [
    "## Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae32ab5-64e9-4fa5-8ce0-a2057108a8f0",
   "metadata": {},
   "source": [
    "### Greedy policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2097d3-180b-4478-8506-5ca776c1f3fe",
   "metadata": {},
   "source": [
    "Given an RDP $\\mathcal{R} = (\\Gamma, V,B)$ and $v\\in V$, a policy $\\sigma\\in\\Sigma$ is called $v$-greedy if\n",
    "\n",
    "$$\n",
    "\\sigma(x)\\in\\arg\\max_{a\\in\\Gamma(x)}B(x,a,v)\n",
    "$$\n",
    "\n",
    "for all $x\\in X$. \n",
    "\n",
    "**Since $\\Gamma(x)$ is finite and nonempty at each $x\\in X$, at least one such policy exists.**\n",
    "\n",
    "We say that $v\\in V$ satisfies the **Bellman equation** if\n",
    "\n",
    "$$\n",
    "v(x)=\\max_{a\\in\\Gamma(x)}B(x,a,v)\n",
    "$$\n",
    "\n",
    "for all $x\\in X$. The **Bellman operator** correponding to $\\mathcal{R}$ is the map $T$ on $V$ defined by\n",
    "\n",
    "$$\n",
    "(Tv)(x) = \\max_{a\\in\\Gamma(x)}B(x,a,x)\n",
    "$$\n",
    "\n",
    "comparing with **policy operator**\n",
    "\n",
    "$$\n",
    "(T_\\sigma v)(x) = B(x,\\sigma(x),v)\n",
    "$$\n",
    "\n",
    "We have \n",
    "\n",
    "1. $Tv = \\bigvee_\\sigma T_\\sigma v$, $T$ is the upper envelope of $\\{T_\\sigma\\}$\n",
    "2. $Tv=T_\\sigma v$ iff $\\sigma$ is $v-greedy$\n",
    "3. $T$ is order-preserving self-map\n",
    "4. $(T^kv)(x) = \\max_{a\\in\\Gamma(x)} B(x,a,T^{k-1}v)$\n",
    "5. $(T_\\sigma^kv)(x) = B(x,\\sigma(x), T^{k-1}_\\sigma v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7a226-ff08-4e39-84a4-355924633ac3",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcfd58-d7f3-49d4-974b-4debe7d3f3e3",
   "metadata": {},
   "source": [
    "OPI is a more practical alternative. HPI is basically the same, just replacing $v_\\sigma = (I-\\beta P)^{-1}r$ to $v_\\sigma$ as the fixed point of $T_\\sigma$. This can be approximate using successive approximation, which lead us to OPI.\n",
    "\n",
    "**OPI: we need to choose the initialized $v_0=v_\\sigma$ for some $\\sigma$.** (Implicitly, this implies $v_0\\le v^*$).\n",
    "\n",
    "OPI reduced to VFI when $m=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24add77c-6805-4801-8862-c234c74c9e56",
   "metadata": {},
   "source": [
    "### Howard operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218fd412-e055-4d6b-9d4f-6e6415c17bba",
   "metadata": {},
   "source": [
    "We call $H: V\\mapsto \\{v_\\sigma\\}_{\\sigma\\in\\Sigma}$ be the **Howard operator** generated by $\\mathcal{R}$, in which,\n",
    "\n",
    "$$\n",
    "Hv = v_\\sigma\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is $v$-greedy. Iterating $H$ implements HPI. \n",
    "\n",
    "In particular, if we fix $\\sigma\\in\\Sigma$ and set $v_k = H^k v_\\sigma$, then $(v_k)_{k\\ge 0}$ is the sequence of $\\sigma$-value functions generated by HPI.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38975269-c1c7-4d9f-8376-ddc193c8904b",
   "metadata": {},
   "source": [
    "### OPI\n",
    "\n",
    "The operator $W_m$ is an approximation of $H$, since $T_\\sigma^m v \\to v_\\sigma = Hv$ as $m\\to\\infty$.\n",
    "\n",
    "Iterating with $W_m$ generates the value sequence in OPI. \n",
    "\n",
    "More specifically, we take $v_0 \\in \\{v_\\sigma\\}$ and generate\n",
    "\n",
    "$$\n",
    "(v_k, \\sigma_k)_{k\\ge 0}\n",
    "$$\n",
    "\n",
    "where $v_k = W^k_m v_0$ and $\\sigma_k$ is $v_k$-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95d0fe-85c4-4f4a-93ea-1011fb177d8f",
   "metadata": {},
   "source": [
    "# Optimality Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b855c-7bfb-4a05-8ef7-19177525361e",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R}$ be a well-posed RDP with policy operator $\\{T_\\sigma\\}$ and $\\sigma$-value functions $\\{v_\\sigma\\}$.\n",
    "\n",
    "We set the **value function as**\n",
    "\n",
    "$$\n",
    "v^* = \\bigvee_{\\sigma\\in\\Sigma} v_\\sigma\n",
    "$$\n",
    "\n",
    "and it satisfies\n",
    "\n",
    "$$\n",
    "v^*(x) = \\max_{\\sigma\\in\\Sigma}v_\\sigma(x)\n",
    "$$\n",
    "\n",
    "A policy $\\sigma$ is called **optimal** for $\\mathcal{R}$ if $v_\\sigma = v^*$ that is if\n",
    "\n",
    "$$\n",
    "v_\\sigma(x)\\ge v_\\tau (x)\n",
    "$$\n",
    "\n",
    "for all $\\tau\\in\\Sigma$ and all $x\\in X$.\n",
    "\n",
    "We say that $\\mathcal{R}$ satisfies **Bellman's principle of optimality** if\n",
    "\n",
    "$$\n",
    "\\sigma\\in\\Sigma \\text{ is optimal for  }\\mathcal{R} \\iff \\sigma \\text{ is $v^*$-greedy}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94962a05-fa68-4ba7-9848-f6e251bebbf2",
   "metadata": {},
   "source": [
    "## Theorem 8.1.1. Optimality Results\n",
    "\n",
    "If $\\mathcal{R}$ is globally stable, then\n",
    "\n",
    "1. $v$^* is the unique solution to the Bellman equation in $V$.\n",
    "\n",
    "2. $\\mathcal{R}$ satisfies Bellman's principle of optimality\n",
    "\n",
    "3. $\\mathcal{R}$ has at least one optimal policy\n",
    "\n",
    "4. HPI returns an optimal policy in finitely many steps\n",
    "\n",
    "5. OPI sequence: $v_k\\to v^*$ as $k\\to\\infty$ and, there exists a $K\\in\\mathbb{N}$ such that $\\sigma_k$ is optimal for all $k\\ge K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056ed67-708d-4ced-bebe-596fddde668a",
   "metadata": {},
   "source": [
    "### Nonstationary policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5716989-ab74-4794-bce4-02a9353d535d",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R} = (\\Gamma, V, B)$ be a globally stable RDP. $(T^k_\\sigma v)(x)$ gives finite horizon utility under policy $\\sigma$ with inital state $x$ and terminal condition $v$.\n",
    "\n",
    "Extending this idea, it is natural to understand $T_{\\sigma_k} T_{\\sigma_{k-1}}\\cdots T_{\\sigma_1} v$ as providing finite horizon utility values for the nonstationary policy sequence $(\\sigma_k)_{k\\in\\mathbb{N}} \\subset\\Sigma$, given terminal condition $v\\in V$.\n",
    "\n",
    "For the same policy sequence we define its lifetime value via\n",
    "\n",
    "$$\n",
    "\\bar v = \\limsup_{k\\to\\infty} v_k \n",
    "$$\n",
    "\n",
    "with $v_k = T_{\\sigma_k} T_{\\sigma_{k-1}}\\cdots T_{\\sigma_1} v$. \n",
    "\n",
    "whenever the limsup is finite and independent of the terminal condition $v$.\n",
    "\n",
    "Under this setting, we let $v\\in V_\\Sigma$. By theorem 8.1.1., we have $T^kv\\to v^*$ as $k\\to\\infty$. \n",
    "\n",
    "Hence, we have,\n",
    "\n",
    "$$\n",
    "\\bar v = \\limsup_{k\\to\\infty} v_k \\le \\limsup_{k\\to\\infty}T^k v = \\lim_{k\\to\\infty} T^kv = v^*\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb12bd8-71bf-4b7a-a2d8-e25b6f7f19b7",
   "metadata": {},
   "source": [
    "## Bounded RDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6289ce-b610-4d9d-83b0-31be0d1b1c07",
   "metadata": {},
   "source": [
    "We call an RDP $\\mathcal{R} = (\\Gamma, V, B)$ **bounded** if $V$ is convex, and moveover, there exists, functions $v_1, v_2$ such that $v_1\\le v_2$,\n",
    "\n",
    "$$\n",
    "v_1(x)\\le B(x,a,v_1), B(x,a,v_2)\\le v_2(x)\n",
    "$$\n",
    "\n",
    "for all $(x,a)\\in G$.\n",
    "\n",
    "We show that boundedness can be used to obtain optimality results for well-posed RDPs even without global stability.\n",
    "\n",
    "** MDP is a bounded RDP. **\n",
    "\n",
    "### Theorem 8.1.2. Well-posed and bounded implies all above optimality result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ac9af-b87a-49a6-b738-4df4a9d884f8",
   "metadata": {},
   "source": [
    "## Topologically conjugate RDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faded26-72b8-42fa-92b6-d4fc2c311f45",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R} = (\\Gamma, V, B)$ and $\\hat{\\mathcal{R}} = (\\Gamma, \\hat V, \\hat B)$ be two RDPs with identical state space $X$, action space $A$ and feasible correspondence $\\Gamma$.\n",
    "\n",
    "We consider settings, where\n",
    "\n",
    "$$\n",
    "V = \\mathbb{M}^X, \\hat V = \\hat{\\mathbb{M}}^X\n",
    "$$\n",
    "\n",
    "where $\\mathbb{M}, \\hat{\\mathbb{M}}\\subset\\mathbb{R}$.  And, in addition, there exists a homeomorphism $\\varphi:\\mathbb{M}\\mapsto \\hat{\\mathbb{M}}$ such that\n",
    "\n",
    "$$\n",
    "B(x,a,v) = \\varphi^{-1}[\\hat B (x,a,\\varphi\\circ v)]\n",
    "$$\n",
    "\n",
    "We call $\\mathcal{R}$ and $\\hat{\\mathcal{R}}$ topologically conjugate under $\\varphi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc70218-0fbe-4dda-a443-e7293f6904a1",
   "metadata": {},
   "source": [
    "## Proposition 8.1.3. $\\mathcal{R}$ is globally stable $\\iff$ $\\hat{\\mathcal{R}}$ is globally stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7c9f9-918f-43f3-8e24-2fd76e2b9106",
   "metadata": {},
   "source": [
    "# Types of RDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53e409-51c7-4686-bfb3-0fd21a0855e8",
   "metadata": {},
   "source": [
    "## Contracting RDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc236ac-8600-4e43-8aee-4c7d3555511f",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R} = (\\Gamma, V, B)$ be an RDP. We call $\\mathcal{R}$ **contracting** if there exists a $\\beta<1$ such that,\n",
    "\n",
    "$$\n",
    "|B(x,a,v)-B(x,a,w)|\\le \\beta \\|v-w\\|_\\infty\n",
    "$$\n",
    "\n",
    "for all $(x,a)\\in G$ and $v,w\\in V$.\n",
    "\n",
    "### Proposition 8.2.1. Contracting RDP implies all policy operators and Bellman operators are contractions with the same modulus of contraction under supremum norm.\n",
    "\n",
    "### Corollary 8.2.2. Contracting RDP with closed $V$ implies RDP is globally stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e10f74-ec75-4b10-b62d-93c528387e0b",
   "metadata": {},
   "source": [
    "### Proposition 8.2.3. Contracting RDP has the following Error Bound\n",
    "\n",
    "Fix $v\\in V$ and let $v_k=T^kv$. If $\\sigma$ is $v_k$-greedy, then\n",
    "\n",
    "$$\n",
    "\\|v^*-v_\\sigma\\|_\\infty\\le \\frac{2\\beta}{1-\\beta}\\|v_k-v_{k-1}\\|_\\infty\n",
    "$$\n",
    "\n",
    "Since VFI terminates when $\\|v_k-v_{k-1}\\|_\\infty$ falls below a given tolerance, this result directly provides a quantitative bound on the performance of the policy returned by VFI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61bc1a-453e-418d-9986-f52bb5298fe2",
   "metadata": {},
   "source": [
    "We say RDP satisfies **Blackwell's condition** if $v\\in V$ implies $v+\\lambda = v+\\lambda\\mathbb{1} \\in V$ for every $\\lambda\\ge 0$ and there exists $\\beta\\in [0,1)$ such that\n",
    "\n",
    "$$\n",
    "B(x,a,v+\\lambda) \\le B(x,a,v) + \\beta\\lambda\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e584c0-8765-4b55-9e98-4cbb55053a7e",
   "metadata": {},
   "source": [
    "## Eventually Contracting RDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616417c-352a-4e90-a87b-5a2dcc2b0b6f",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R} = (\\Gamma, V, B)$ be an RDP with policy set $\\Sigma$. We call $\\mathcal{R}$ **eventually contracting** if there is a map $L:G\\times X\\mapsto \\mathbb{R}_+$ such that,\n",
    "\n",
    "$$\n",
    "|B(x,a,v)-B(x,a,w)|\\le \\sum_{x'} |v(x')-w(x')|L(x,a,x')\n",
    "$$\n",
    "\n",
    "for all $(x,a)\\in G$ and all $v,w\\in V$ and moreover,\n",
    "\n",
    "$$\n",
    "\\sigma\\in\\Sigma \\implies \\rho(L_\\sigma)<1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb105afe-87ce-42c4-bd9c-a0c008f74a2c",
   "metadata": {},
   "source": [
    "### Proposition 8.2.4. RDP is eventually contracting on closed set implies gloablly stable RDP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7be94c-81e8-4221-9914-a0b6278be2a3",
   "metadata": {},
   "source": [
    "## Convex and Concave RDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af28f9-1db2-4b07-9fb6-1e24dc0ba158",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R} = (\\Gamma, V, B)$ be an RDP with $V=[v_1,v_2]$ for some $v_1\\le v_2$ in\\mathbb{R}^X. \n",
    "\n",
    "We call $\\mathcal{R}$ **convex** if\n",
    "\n",
    "- for all $(x,a)\\in G$, $\\lambda\\in[0,1]$ and $v,w\\in V$, we have,\n",
    "\n",
    "$$\n",
    "B(x,a,\\lambda v + (1-\\lambda)w) \\le \\lambda B(x,a,v) + (1-\\lambda)B(x,a,w)\n",
    "$$\n",
    "\n",
    "**and**\n",
    "\n",
    "- there exists a $\\delta>0$ such that\n",
    "\n",
    "$$\n",
    "B(x,a,v_2) \\le v_2(x)-\\delta[v_2(x)-v_1(x)]\n",
    "$$\n",
    "\n",
    "for all $(x,a)\\in G$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb39f3-2c29-4f01-b418-1d2e13136dd1",
   "metadata": {},
   "source": [
    "We call $\\mathcal{R}$ concave if\n",
    "\n",
    "- for all $(x,a)\\in G$, $\\lambda\\in[0,1]$, $v,w\\in V$, we have,\n",
    "\n",
    "$$\n",
    "B(x,a,\\lambda v+(1-\\lambda)w)\\ge \\lambda B(x,a,v)+(1-\\lambda) B(x,a,w)\n",
    "$$\n",
    "\n",
    "**and**\n",
    "\n",
    "- there exists a $\\delta>0$ such that\n",
    "\n",
    "$$\n",
    "B(x,a,v_1) \\ge v_1(x) + \\delta [v_2(x)-v_1(x)]\n",
    "$$\n",
    "\n",
    "for all $(x,a)\\in G$.\n",
    "\n",
    "\n",
    "### Proposition 8.2.5. Convex or concave RDP implies globally stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a48ce3-f66e-4afc-8af9-cd2cf866cfb8",
   "metadata": {},
   "source": [
    "# Further Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afcb89e-be8d-4b54-b564-4f5460b061e5",
   "metadata": {},
   "source": [
    "## Risk-sensitive RDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab88667-fffd-4cd0-862f-4d45147b81f7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
