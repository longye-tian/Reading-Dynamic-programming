{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8a4745-ac26-4ce5-877d-0b31557e3680",
   "metadata": {},
   "source": [
    "# Chapter 4 Optimal Stopping Reading Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156edf3-8a27-475f-be3b-3660661437d9",
   "metadata": {},
   "source": [
    "1. Theory of Optimal Stopping: \n",
    "   - Theory: The stopping problem, Lifetime values, Policy operators, the value functions, the Bellman Operator, Optimal policies, value function iteration\n",
    "   - Firm Valuation and Exit: Optional exit, exit vs no-exit\n",
    "   - Monotonicity: monotone values, monotone actions\n",
    "   - Continuation values: continuation value operator, dimensionality reduction, application to firm value\n",
    "\n",
    "2. Applications:\n",
    "   - American Options\n",
    "   - Research and Development: Constant R&D costs, IID R&D costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50216f9-cff6-4b88-b556-f40c0d34c6cb",
   "metadata": {},
   "source": [
    "## Theorems\n",
    "\n",
    "1. $T_\\sigma$ is a contraction\n",
    "2. $T$ is a contraction, $v^*$ is the unique fixed point\n",
    "3. Optimal policy $\\sigma^*$ is the $v^*$-greedy policy\n",
    "4. $e,c$ increasing and $P$ monotone increasing implies $v^*,h^*$ increasing\n",
    "5. $e$ increasing, $h^*$ decreasing implies $\\sigma^*$ increasing\n",
    "6. $e$ decreasing, $h^*$ increasing implies $\\sigma^*$ decreasing\n",
    "7. continuation value operator is a contraction with unique fixed point $h^*\\in\\mathbb{R}^X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ceeb61-125c-45e3-b4a6-d72c1b345ba1",
   "metadata": {},
   "source": [
    "### Proposition 4.1.1. $T_\\sigma$ is a contraction\n",
    "\n",
    "For any $\\sigma\\in\\Sigma$, the policy operator $T_\\sigma$ is a contraction of modulus $\\beta$ on $\\mathbb{R}^X$ **under the supremum norm**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa641d-56e4-4787-be35-22aebf843058",
   "metadata": {},
   "source": [
    "### Proposition 4.1.2. $T$ is a contraction, $v^*$ is the unique fixed point\n",
    "\n",
    "If $\\mathscr{S}$ is an optimal stopping problem with Bellman operator $T$ and value function $v^*$, then\n",
    "\n",
    "1. $T$ is a contraction of modulus $\\beta$ on $\\mathbb{R}^X$ under the supremum norm\n",
    "2. The unique fixed point of $T$ on $\\mathbb{R}^X$ is the value function $v^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c74d92-b876-499b-9146-946d31625336",
   "metadata": {},
   "source": [
    "### Proposition 4.1.3. Optimal policy $\\sigma^*$ is $v^*$-greedy policy\n",
    "\n",
    "Policy $\\sigma\\in\\Sigma$ is optimal if and only if it is $v^*$-greedy policy.\n",
    "\n",
    "(**A version of Bellman's principle of optimality**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a435e-278f-4f94-94a7-5da79086553c",
   "metadata": {},
   "source": [
    "### Lemma 4.1.4. \n",
    "\n",
    "If $e,c\\in i\\mathbb{R}^X$ and $P$ is monotone increasing, then $h^*$ and $v^*$ are both increasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a62e32-e741-4aaa-9568-e54251785ba4",
   "metadata": {},
   "source": [
    "### Proposition 4.1.5. Continuation value operator is a contraction with unique fixed point\n",
    "\n",
    "The continuation value operator $C$ is a contraction of modulus $\\beta$ on $\\mathbb{R}^X$ with unique fixed point $h^*\\in\\mathbb{R}^X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97068a8d-29b4-4703-982a-f632fddede15",
   "metadata": {},
   "source": [
    "### The Stopping Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6882f-0494-4492-93eb-eddf0a5a8a75",
   "metadata": {},
   "source": [
    "- An entrepreneur who decides to enter or exit a market\n",
    "- A borrower who considers defaulting on a loan\n",
    "- A firm that contemplates introducing a new technology\n",
    "- A portfolio manager decide whether to exercise a real or financial option.\n",
    "\n",
    "They are all two-action (or binary choice) problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd97b47-1405-453c-b51d-0053f66aca0d",
   "metadata": {},
   "source": [
    "### Optimal Stopping Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896299c3-df6d-41f8-8dff-d5a0ff1486a6",
   "metadata": {},
   "source": [
    "Let $X$ be a finite set. Given $X$, an **optimal stopping problem** is a 4-tuple $\\mathscr{S} = (\\beta, P, r,e)$ that consists of,\n",
    "\n",
    "1. a discount factor $\\beta\\in(0,1)$\n",
    "2. a Markov operator $P\\in\\mathscr{M}(\\mathbb{R}^X)$\n",
    "3. a **continuation reward function** $c\\in\\mathbb{R}^X$\n",
    "4. an **exit reward function** $e\\in\\mathbb{R}^X$\n",
    "\n",
    "Given a $P$-Markov chain $(X_t)_{t\\ge 0}$, a decision maker observes the state $X_t$ in each period and decides whether to continue or stop.\n",
    "\n",
    "- If she chooses to stop, she receives final reward $e(X_t)$ and the process terminates.\n",
    "\n",
    "- If she chooses to continue, then she receives $c(X_t)$ and the process repeats next period.\n",
    "\n",
    "\n",
    "### Lifetime reward\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\sum_{t\\ge 0} \\beta^t R_t\n",
    "$$\n",
    "\n",
    "- If she chooses continue, then $R_t =c(X_t)$ while the agent continues,\n",
    "\n",
    "- If she chooses stop, then $R_t = e(X_t)$ and zero afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f257fa3-1143-4dc4-95f5-74ee91917e5c",
   "metadata": {},
   "source": [
    "### Policy function\n",
    "\n",
    "The optimal decisions are described by **policy function** which is a map \n",
    "\n",
    "$$\n",
    "\\sigma:X\\mapsto\\{0,1\\}\n",
    "$$\n",
    "\n",
    "After observing state $x$ at any given time, the decision maker takes action $\\sigma(x)$, where $0$ means to continue and $1$ means to stop.\n",
    "\n",
    "**Implicit in this formulation is we assume that the current state contains enough information for the agent to decide whether or not to stop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c9b97-e739-4062-96e5-ecd464f4a790",
   "metadata": {},
   "source": [
    "### $\\sigma$-value function\n",
    "\n",
    "Let $\\Sigma$ be the set of functions from $X$ to $\\{0,1\\}$ (**Policy function space**).\n",
    "\n",
    "Let $v_\\sigma(x)$ denote the expected lifetime value of following policy $\\sigma$ now and in every future period given optimal stopping problem $\\mathscr{S}(\\beta,P,r,e)$ and current state $x\\in X$.\n",
    "\n",
    "We call $v_\\sigma$ the **$\\sigma$-value function**. We also call $v_\\sigma(x)$ the **lifetime value of policy $\\sigma$ conditional on initial state $x$**. \n",
    "\n",
    "A policy $\\sigma^*\\in\\Sigma$ is called **optimal** for $\\mathscr{S}$ if\n",
    "\n",
    "$$\n",
    "v_{\\sigma^*}(x)=\\max_{\\sigma\\in\\Sigma}v_{\\sigma}(x)\n",
    "$$\n",
    "\n",
    "for all $x\\in X$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eceef8c-d123-40a2-8a82-30d65cd931e9",
   "metadata": {},
   "source": [
    "### Lifetime values\n",
    "\n",
    "Fix $\\sigma\\in\\Sigma$, let's consider how to compute the lifetime value $v_\\sigma(x)$ of following $\\sigma$ conditional on $X_0=x$.\n",
    "\n",
    "$v_\\sigma$ satisfies:\n",
    "\n",
    "$$\n",
    "v_\\sigma(x)=\\sigma(x)e(x) + (1-\\sigma(x))\\left[c(x)+\\beta\\sum_{x'\\in X} v_\\sigma(x')P(x,x')\\right] \\tag{1}\n",
    "$$\n",
    "\n",
    "for all $x\\in X$.\n",
    "\n",
    "**The value of continuing is the current reward plus the discounted expected reward obtain by continuing with policy $\\sigma$ next period**.\n",
    "\n",
    "### Solve (1)\n",
    "\n",
    "Define $r_\\sigma\\in\\mathbb{R}^X$ and $L_\\sigma \\in\\mathscr{L}(\\mathbb{R}^X)$ via\n",
    "\n",
    "$$\n",
    "r_\\sigma(x):= \\sigma(x)e(x)+(1-\\sigma(x)) c(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_\\sigma(x,x') = \\beta(1-\\sigma(x))P(x,x')\n",
    "$$\n",
    "\n",
    "With this notation, we have,\n",
    "\n",
    "$$\n",
    "v_\\sigma = r_\\sigma + L_\\sigma v_\\sigma\n",
    "$$\n",
    "\n",
    "If $\\rho(L_\\sigma)<1$, we have, $v_\\sigma$ is **uniquely defined by**\n",
    "\n",
    "$$\n",
    "v_\\sigma = (I-L_\\sigma)^{-1} r_\\sigma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d003eb-0d07-4153-82c9-4818eceb0c4c",
   "metadata": {},
   "source": [
    "### Policy Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1617280-8261-4b07-a752-ec5ae37f29b0",
   "metadata": {},
   "source": [
    "**It is helpful to view $v_\\sigma$ as the fixed point of an operator**.\n",
    "\n",
    "We associate each $\\sigma\\in \\Sigma$ with an **Policy Operator $T_\\sigma$** defined at $v\\in \\mathbb{R}^X$ by\n",
    "\n",
    "$$\n",
    "(T_\\sigma v)(x) = \\sigma(x)e(x)+(1-\\sigma(x))\\left[c(x)+\\beta\\sum_{x'\\in X}v(x') P(x,x')\\right]\n",
    "$$\n",
    "\n",
    "for each $x\\in X$. \n",
    "\n",
    "$v_\\sigma$ is the fixed point of $T_\\sigma$, i.e., \n",
    "\n",
    "$$\n",
    "T_\\sigma v_\\sigma =v_\\sigma\n",
    "$$\n",
    "\n",
    "$T_\\sigma$ is a contraction, this implies that $v_\\sigma$ is the unique fixed point of $T_\\sigma$ in $\\mathbb{R}^X$. Iterating $T_\\sigma$ always converges to $v_\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2b52c-200f-42c5-9cdc-25409012bf4b",
   "metadata": {},
   "source": [
    "### Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f147bfb-6f0f-47ea-900d-7e9a978d09d9",
   "metadata": {},
   "source": [
    "Given an optimal stopping problem $S=(\\beta, P, r,e)$ with $\\sigma$-value functions $\\{v_\\sigma\\}_{\\sigma\\in\\Sigma}$, we define the **value function** $v^*$ of $\\mathscr{S}$ via\n",
    "\n",
    "$$\n",
    "v^*(x) := \\max_{\\sigma\\in\\Sigma} v_\\sigma(x)\n",
    "$$\n",
    "\n",
    "for all $x\\in X$.\n",
    "\n",
    "So that $v^*(x)$ is the maximal lifetime value available to an agent facing current state $x$.\n",
    "\n",
    "We write $v^* =\\bigvee_\\sigma v_\\sigma$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f304f-7b04-481d-85ed-260081ebc79b",
   "metadata": {},
   "source": [
    "## Steps to obtain the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb3afe-dcdd-4d6b-8459-07aac91b57ee",
   "metadata": {},
   "source": [
    "1. Formulate a **Bellman euqation** for the value function of the optimal stopping problem, namely\n",
    "\n",
    "$$\n",
    "v(x) =\\max\\{e(x),c(x)+\\beta\\sum_{x'\\in X}v(x)P(x,x')\\} \\tag{$x\\in X$}\n",
    "$$\n",
    "\n",
    "2. Prove that this Bellman equation **has a unique solution in $\\mathbb{R}^X$**\n",
    "\n",
    "3. Show that this unique solution equals the value function $v^*(x)=\\max_{\\sigma\\in\\Sigma} v_\\sigma(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da2c73-d032-4358-8f37-aaf719f81d8d",
   "metadata": {},
   "source": [
    "### Bellman Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363aca31-2d4a-40ac-953f-87d26478084e",
   "metadata": {},
   "source": [
    "Define the **Bellman Operator** for the optimal stopping problem $\\mathscr{S}=(\\beta, P,r,e)$ as\n",
    "\n",
    "$$\n",
    "(Tv)(x)=\\max\\left\\{e(x), c(x)+\\beta\\sum_{x'}v(x')P(x,x')\\right\\}\n",
    "$$\n",
    "\n",
    "where $x\\in X$, $v\\in\\mathbb{R}^X$.\n",
    "\n",
    "By construction, any fixed point of $T$ solves the Bellman equation and vice versa.\n",
    "\n",
    "Pointwise, we can express $T$ via $Tv = e\\vee (c+\\beta Pv)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa7ad75-4961-4671-9f2d-e4cb65780937",
   "metadata": {},
   "source": [
    "**Proof strategy of Proposition 4.1.2. looks important**\n",
    "\n",
    "We want to show that the unique fixed point of $T$, $\\bar v$ is $v^*$ by showing\n",
    "\n",
    "1. $\\bar v\\le v^*$\n",
    "2. $v^*\\le \\bar v$\n",
    "\n",
    "Part 1:\n",
    "\n",
    "$\\bar v$ induces a $\\bar v$-greedy policy, $\\sigma$. This $\\sigma$ induces a Bellman operator $T_\\sigma$. Then we can show that $\\bar v$ is the fixed point of $T_\\sigma$. Since $T_\\sigma$ has unique fixed point, we have $\\bar v =v_\\sigma$. Since $v^*=\\bigvee_\\sigma v_\\sigma$. We have $\\bar v\\le v^*$.\n",
    "\n",
    "Part 2:\n",
    "\n",
    "Since $\\bar v$ is the unique fixed point of a globally stable operator $T$ and we have $T$ dominates $T_\\sigma$ for all $\\sigma\\in\\Sigma$ by definition. Hence, this implies the unique fixed point dominates all the fixed point of $T_\\sigma$ for all $\\sigma\\in \\Sigma$. This implies $v^*=\\bigvee_\\sigma v_\\sigma \\le \\bar v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5631ba92-a36f-4e5d-bd8f-5e10735a504b",
   "metadata": {},
   "source": [
    "### Optimal policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2e17c-7720-48ad-b206-a3e77fe86fdf",
   "metadata": {},
   "source": [
    "For each $v\\in\\mathbb{R}^X$, we call $\\sigma\\in\\Sigma$ **v-greedy policy** if for all $x\\in X$, we have,\n",
    "\n",
    "$$\n",
    "\\sigma(x)\\in \\arg\\max_{a\\in\\{0,1\\}}\\left\\{ae(x)+(1-a)\\left[c(x)+\\beta\\sum_{x'\\in X}v(x')P(x,x')\\right]\\right\\}\n",
    "$$\n",
    "\n",
    "**A $v$-greedy policy uses $v$ to assign values to states and then chooses to stop or continue based on the action that generates a higher payoff**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb7519-cf4a-43e8-99b1-c172fefc1672",
   "metadata": {},
   "source": [
    "## Firm valuation with EXIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff86a2-db89-468a-b7b2-a4acda75e7a3",
   "metadata": {},
   "source": [
    "ADD EXIT OPTION:\n",
    "\n",
    "Firm now have the option to cease operations and sell all remaining assets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd8fec-bc0a-41d5-bc04-e641a9f1f4c7",
   "metadata": {},
   "source": [
    "### Optional Exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85155513-4fc0-4cf3-a356-9125a4955673",
   "metadata": {},
   "source": [
    "Consider a firm with\n",
    "- exogenous $Q$-Markov productivity: $(Z_t)_{t\\ge 0}$, $Z$ finite.\n",
    "\n",
    "- fixed profit function productivity: $\\pi_t= \\pi(Z_t)$\n",
    "\n",
    "- At the start of each period, firm choose to continue or exit:\n",
    "  - continue: receive $\\pi_t$\n",
    "  - exit: receive $s>0$, the scrape value of the firm\n",
    "- Discount factor at $\\beta = 1/(1+r)$, where $r$ is the interest rate, $r>0$\n",
    "\n",
    "- $\\Sigma = Z^{\\{0,1\\}}$: policy function space\n",
    "\n",
    "- Policy operator:\n",
    "\n",
    "$$\n",
    "(T_\\sigma v)(z) = \\sigma(z)s+(1-\\sigma(z))\\left[\\pi(z)+ \\frac{1}{1+r}\\sum_{z'\\in Z} v(z') Q(z,z')\\right]\\tag{$z\\in Z$}\n",
    "$$\n",
    "\n",
    "- $v_\\sigma(z)$ is the unique fixed point of $T_\\sigma$, representing the value of following $\\sigma$ and conditional on the initial state is $Z_0=z$.\n",
    "\n",
    "- **Bellman operator** $T$ is\n",
    "\n",
    "$$\n",
    "(Tv)(z) =\\max\\left\\{s, \\pi(z) +\\beta\\sum_{z'\\in Z}v(z')Q(z,z')\\right\\}\\tag{$(z\\in Z)$}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "Tv = s\\vee(\\pi + \\beta Qv)\n",
    "$$\n",
    "\n",
    "- **Stopping value**: s\n",
    "- **Continuation value function**: \n",
    "\n",
    "$$\n",
    "h^*(z) = \\pi(z)+\\beta\\sum_{z'\\in Z}v^*(z')Q(z,z')\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "h^* = \\pi+\\beta Qv^*\n",
    "$$\n",
    "\n",
    "\n",
    "Using successive approximation, we get $v^*$ and we get $v^*$-greedy policy $\\sigma^*$. **The $v^*$-greedy policy instruct the firm to exit when the continuation value is less than the scrap value**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd664a6-6c05-4496-8ce3-99365301ca4e",
   "metadata": {},
   "source": [
    "### EXIT vs NO EXIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eecd50-02e3-4ed3-a5cd-e0b5cfccd82b",
   "metadata": {},
   "source": [
    "We define \n",
    "\n",
    "$$\n",
    "w(z) = \\mathbb{E}_z\\sum_{t\\ge 0}\\beta^t \\pi_t\\tag{$z\\in Z$}\n",
    "$$\n",
    "\n",
    "$w(z)$ is the value of the firm given $Z_0=z$ when **firm never exits**.\n",
    "\n",
    "**Intuitive proof**\n",
    "\n",
    "Let $\\sigma_0$ denote the policy of never exit, hence $\\sigma_0\\in\\Sigma$. Since $v^*=\\bigvee_\\sigma v_\\sigma$. We have $v^*\\ge v_{\\sigma_0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf719a4-5b42-4fbe-9d94-6e8901472e45",
   "metadata": {},
   "source": [
    "## Monotonicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c0670-4db8-4574-b9d7-f333582ddfb0",
   "metadata": {},
   "source": [
    "### Monotone values\n",
    "\n",
    "Let $v^*$ be the value function of an optimal stopping problem defined by $X,P,\\beta,c,e$ and defined a **continuation value function** $h^*$\n",
    "\n",
    "$$\n",
    "h^*(x) = c(x)+\\beta\\sum_{x'\\in X} v^*(x')P(x,x') \\tag{$x\\in X$}\n",
    "$$\n",
    "\n",
    "Let $X$ be partially ordered and let $i\\mathbb{R}^X$ be the increasing function on $i\\mathbb{R}^X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7081e-599d-46ba-a8d8-5c6e6380d982",
   "metadata": {},
   "source": [
    "If $e,c\\in i\\mathbb{R}^X$, $P$ monotone increasing, $h^*,v^*\\in i\\mathbb{R}^X$. \n",
    "\n",
    "Proof by $P$ is monotone implies invariant on $i\\mathbb{R}^X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506d71f-c971-49d1-88ed-03985944ff96",
   "metadata": {},
   "source": [
    "### Monotone actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ba5d5-7df9-4abf-9f5a-32d78ea2974c",
   "metadata": {},
   "source": [
    "Take $X\\subset \\mathbb{R}$, finite, and ordered by $\\le$. \n",
    "\n",
    "**Sufficient not necessary conditions for monotone actions**:\n",
    "\n",
    "- If $e$ decreasing, $h^*$ increasing, this implies $\\sigma^*$ decreasing.\n",
    "- If $e$ increasing, $h^*$ decreasing, this implies $\\sigma^*$ increasing.\n",
    "\n",
    "For a binary function on $X\\subset \\mathbb{R}$, the condition that \n",
    "\n",
    "- **$\\sigma^*$ is decreasing means that the decision maker chooses to exit when $x$ is sufficiently small**.\n",
    "\n",
    "- **$\\sigma^*$ is increasing means that the decision maker chooses to exit when $x$ is sufficiently large**.\n",
    "\n",
    "This applies to the firm exit problem: Since $Q$ is monotone increasing, low current value of $z$ predict low future values of $z$ (low expectation). So profits associated with continuing can be anticipated to be low.\n",
    "\n",
    "\n",
    "**Since $X\\subset \\mathbb{R}$ is totally ordered, monotonicity implies that a threshold policy is optimal**, i.e., we take $x^*$ to be the smallest $x\\in X$ such that $\\sigma^*(x)=1$, for such $x^*$, we have\n",
    "\n",
    "$$\n",
    "x<x^* \\implies \\sigma^*(x)=0, x>x^*\\implies \\sigma^*(x)=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e2b5a-0574-4dcc-8a9b-f1956e8b9e96",
   "metadata": {},
   "source": [
    "## Continuation values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e48919-06a2-42a2-9121-dab024720928",
   "metadata": {},
   "source": [
    "While all relevant state components must be included in the value function, purely transitory components do no affect continuation values. Hence, the continuation value approach is at least as efficient and sometimes substantially more so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4691a7-6acf-433e-8133-5ed2c1bdc00b",
   "metadata": {},
   "source": [
    "### The continuation value operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4759b0e-f6e1-4396-af72-4115284fb509",
   "metadata": {},
   "source": [
    "Let $h^*$ be the continuation value function for the optimal stopping problem. To compute $h^*$ directly we begin with the optimal stopping version of the Bellman equation evaluated at $v^*$ and writed as \n",
    "\n",
    "$$\n",
    "v^*(x') = \\max\\{e(x'),h^*(x')\\}\\tag{$x'\\in X$}\n",
    "$$\n",
    "\n",
    "By the formula of continuation value function, we have,\n",
    "\n",
    "\\begin{align*}\n",
    "h^*(x) &= c(x)+\\beta\\sum_{x'\\in X}v^*(x')P(x,x')\\\\\n",
    "&=c(x)+\\beta\\sum_{x'\\in X} \\max\\{e(x'),h^*(x')\\}P(x,x')\\tag{$x\\in X$}\n",
    "\\end{align*}\n",
    "\n",
    "This expression motivates us to introduce a **continuation value operator** $C:\\mathbb{R}^X\\mapsto \\mathbb{R}^X$ via\n",
    "\n",
    "$$\n",
    "(Ch)(x) = c(x)+\\beta\\sum_{x'\\in X}\\max\\{e(x'), h(x')\\}P(x,x') \\tag{$x\\in X$}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "Ch = c+\\beta P(e\\vee h)\n",
    "$$\n",
    "\n",
    "The continuation value operator is a contraction with a unique fixed point $h^*\\in\\mathbb{R}^X$. \n",
    "\n",
    "This implies we have the following **algorithm to compute optimal policy:**\n",
    "\n",
    "1. Use successive approximation to find $h^*$\n",
    "2. Calculate $\\sigma^*(x)=\\mathbb{1}\\{e(x)\\ge h^*(x)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8324743-10c2-445d-a7d9-ebf976455d59",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6788dce-70a3-4152-b99c-0edac5b5edc2",
   "metadata": {},
   "source": [
    "Continuation value iteration can substantially reduce the dimensionality of the problem in some cases. \n",
    "\n",
    "Let $W,Z$ be two finite sets and suppose that $\\varphi\\in\\mathcal{D}(W)$ iid and $Q\\in \\mathscr{M}(\\mathbb{R}^Z)$ be an $Q$-Markov chain on $Z$.\n",
    "\n",
    "**If W,Z are independent, then $(X_t)$ defined by $X_t = (W_t, Z_t)$  is $P$-Markov on $X$**, where,\n",
    "\n",
    "$$\n",
    "P(x,x') = P((w,z),(w',z')) = \\varphi(w')Q(z,z')\n",
    "$$\n",
    "\n",
    "Suppose that the continuation reward depends only on $z$ so that we can write the Bellman operator as\n",
    "\n",
    "$$\n",
    "(Tv)(w,z)=\\max\\left\\{e(w,z), c(z)+\\beta\\sum_{w'\\in W}\\sum_{z'\\in Z} v(w',z') \\varphi(w') Q(z,z')\\right\\}\n",
    "$$\n",
    "\n",
    "Since the right-hand side depends on both $w$ and $z$, the Bellman operator acts on an $n$-dimensional spaces where $n:=|X|=|W|\\times |Z|$.\n",
    "\n",
    "However, since the continuation value function only depends on $z$ because dependence on $w'$ vanishes because $w$ does not help predict $w'$.\n",
    "\n",
    "Thus, the continuation value function is an object in $|Z|$-dimensional space.\n",
    "\n",
    "The continuation value operator\n",
    "\n",
    "$$\n",
    "(Ch)(z) = c(z)+\\beta\\sum_{w'\\in W}\\sum_{z'\\in Z} \\max\\{e(w',z'), h^*(z')\\}\\varphi(w')Q(z,z')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e77de-a736-4db4-819c-3c72fd9979f6",
   "metadata": {},
   "source": [
    "## Application: American Call Option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd603ee-3d2e-4b4d-8d62-c27036997a45",
   "metadata": {},
   "source": [
    "American Call Option: provide the right to buy a particular stock or bond at fixed **strike price** $K$ at any time befire a set expiration date.  The market price of the asset at time $t$ is denoted by $S_t$.\n",
    "\n",
    "We are interested in **computing the expected value of holding the option when discounting with a fixed interest rate**\n",
    "\n",
    "Finite horizon American options can be priced by \n",
    "- backward induction\n",
    "- embed finite horizon options into the thoery of infinite horizon optimal stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b22fa-4052-40a0-ba55-7f76664d43ea",
   "metadata": {},
   "source": [
    "We take\n",
    "- $T\\in\\mathbb{N}$ be the expiration date. \n",
    "- Option purchased at $t=0$ and can be exercised at any $t\\le T$.\n",
    "- We set:\n",
    "   - $\\top:=\\{1, \\cdots, T+1\\}$, $m(t):=\\min\\{t+1, T+1\\}$ for all $t\\in \\top$\n",
    "   - (Time is updated via $t'=m(t)$, so that time increments at each update until $t=T+1$, after that we hold $t$ constant, bounding time at $T+1$ keeps the state space finite)\n",
    "\n",
    "- Stock price $S_t$ evolves according to\n",
    "   - $S_t = W_t + Z_t$\n",
    "   - $(W_t)_{t\\ge 0} \\sim_{IID} \\mathcal{D}(W)$\n",
    "   - $(Z_t)_{t\\ge 0}$ is $Q$-Markov for some $Q\\in\\mathscr{M}(\\mathbb{R}^Z)$\n",
    "   - This implies the share price has both persistent and transcient stochastic components\n",
    "   \n",
    "- State space $X:= \\top \\times W\\times Z$\n",
    "\n",
    "- $(X_t)$ is a $P$-Markov with\n",
    "\n",
    "$$\n",
    "P((t,w,z),(t',w',z')):=\\mathbb{t'=m(t)}\\varphi(w')Q(z,z')\n",
    "$$\n",
    "   - time updates deterministically via $t'=m(t)$\n",
    "   - $z',w'$ are drawn independently from $Q(z,\\cdot),\\varphi$\n",
    "- Continuation reward is zero\n",
    "- Discount factor $\\beta = 1/(1+r), r>0$ is fixed risk-free rate.\n",
    "\n",
    "- Exit(Exercising) reward: $\\mathbb{1}\\{t\\le T\\}(S_t-K)$ so that exercising at time $t$ earns the owner $S_t-K$ up to expiration date and zero afterwards.\n",
    "\n",
    "   - In terms of the state $(t,z)$, the exit reward is\n",
    "   $$\n",
    "   e(t,w,z):= \\mathbb{1}\\{t\\le T\\}[z+w-K]\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00deb7-85b1-4430-aa20-6157245d6fa7",
   "metadata": {},
   "source": [
    "**Bellman equation**\n",
    "\n",
    "$$\n",
    "v(t,w,z)=\\max\\{e(t,w,z), \\beta\\sum_{w'\\in W}\\sum_{z'\\in Z}v(t', w',z')\\varphi(w') Q(z,z')\\}\n",
    "$$\n",
    "\n",
    "where $t'=m(t)$.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The value of the option is the maximum of current exercise value and the discounted expected value of carrying the option over the next period.\n",
    "\n",
    "This is an optimal stopping problem. Hence, iterating the Bellman operator leads to a unique fixed point $v^*$ and the policy is optimal if and only if it is $v^*$-greedy (Bellman's principle of optimality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd056da-c02c-4bbb-8733-df668353c1be",
   "metadata": {},
   "source": [
    "### Continuation value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e06b4a-79f7-41ec-9585-38accb68defb",
   "metadata": {},
   "source": [
    "We can do better than value function iteration, as $(W_t)\\sim_{IID}\\varphi$. We define the continuation value operator as\n",
    "\n",
    "$$\n",
    "(Ch)(t,z)=\\beta\\sum_{z'\\in Z}\\sum_{w'\\in W}\\max\\{e(t',w',z'), h(t,z)\\}\\varphi(w')Q(z,z')\n",
    "$$\n",
    "\n",
    "As proved before, the continuation value operator is a contraction with a unique fixed point $h^*\\in\\mathbb{R}^X$.\n",
    "\n",
    "After obtaining $h^*$, we can compute the optimal policy as\n",
    "\n",
    "$$\n",
    "\\sigma^*(t,w,z) =\\mathbb{1}\\{e(t,w,z)\\ge h^*(t,z)\\}\n",
    "$$\n",
    "\n",
    "where $\\sigma^*(t,w,z)=1$ implying exercising the option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8aa53c-40b9-471f-8805-2518615b8183",
   "metadata": {},
   "source": [
    "## Application: Research and Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bda690-5850-4df8-831f-409da5182d8c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
