{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c14285-1b98-4136-a5e3-9f79819fa337",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998283b-b2e1-4a86-b42c-5ecad67840bd",
   "metadata": {},
   "source": [
    "**Definition (State)**\n",
    "\n",
    "The state variable $X_t$ is a vector listing current values of variables relevant to choosing the current action.\n",
    "\n",
    "**Definition (Value function)**\n",
    "\n",
    "The value function $v_t(w)$ track the optimal lifetime payoffs from a given state at a given time. It depends only on the parameters (think about the *indirect utility function*). \n",
    "\n",
    "**Definition (Bellman equation)**\n",
    "\n",
    "**Definition (Bellman's Principle of Optimality)**\n",
    "\n",
    "**Definition (Globally Stable)**\n",
    "\n",
    "A self-map $T$ is **globally stable** on $U\\subset \\mathbb{R}^n$ if \n",
    "- $T$ has a unique fixed point $x^*\\in U$\n",
    "- $T^ku\\to u^*$ as $k\\to\\infty$ for all $u\\in U$\n",
    "\n",
    "**Definition (Invariant)**\n",
    "\n",
    "Let $T$ be a self-map on $U\\subset\\mathbb{R}^n$. $C\\subset U$ is **invariant** if \n",
    "\n",
    "$$\\forall u\\in C, Tu\\in C$$\n",
    "\n",
    "($T$ is a self-map on $C$ also.)\n",
    "\n",
    "**Definition (Contraction)**\n",
    " \n",
    "Let $T$ be a self-map on $U\\subset\\mathbb{R}^n, U\\neq\\emptyset$ with norm $\\|\\cdot\\|$. $T$ is a contraction on $U$ if there exists $\\lambda<1$ such that,\n",
    "\n",
    "$$\n",
    "\\|Tu-Tv\\| \\le \\lambda\\|u-v\\| \\,\\,\\forall u,v\\in U\n",
    "$$\n",
    "\n",
    "**Definition (Distribution)**\n",
    "\n",
    "Given finite $X$, the set of distributions on $X$ is \n",
    "\n",
    "$$\n",
    "\\mathcal{D}(X):= \\left\\{\\varphi\\in\\mathbb{R}^X:\\varphi\\ge 0,\\,\\,\\sum_{x\\in X}\\varphi(x) = 1\\right\\}\n",
    "$$\n",
    "\n",
    "We say a random variable $Y$ has a distribution $\\varphi\\in\\mathcal{D}(X)$ if\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{Y=x\\} = \\varphi(x)\\,\\,\\forall x\\in X\n",
    "$$\n",
    "\n",
    "**Definition (Expectation)**\n",
    "For $h\\in\\mathbb{R}^X and Y\\sim \\varphi$, the expectation of $h(Y)$ is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}h(Y):= \\sum_{x\\in X} h(x)\\varphi(x) = \\langle h,\\varphi\\rangle\n",
    "$$\n",
    "\n",
    "**Definition (Cumulative distribution function)**\n",
    "\n",
    "If $X\\subset \\mathbb{R}$ (still finite), then,\n",
    "\n",
    "$$\n",
    "\\Phi(x):= \\mathbb{P}\\{X\\le x\\} =\\sum_{x'\\in X} \\mathbb{1}\\{x'\\le x\\}\\varphi(x')\n",
    "$$\n",
    "is the cumulative distribution function(CDF).\n",
    "\n",
    "**Definition (Quantile) (Median)**\n",
    "\n",
    "If $\\tau\\in[0,1]$, then the $\\tau-th$ quantile of $Y$ is\n",
    "\n",
    "$$\n",
    "Q_tau Y:= \\min\\{x\\in X: \\Phi(x\\ge \\tau)\\}\n",
    "$$\n",
    "\n",
    "Median is when $\\tau = 1/2$.\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Let $\\{v_\\sigma\\}_{\\sigma\\in\\Sigma}$ be a finite subset of $\\mathbb{R}^X$. We set\n",
    "\n",
    "$$\n",
    "\\bigwedge_\\sigma v_\\sigma (x):= \\min_{\\sigma\\in\\Sigma} v_\\sigma(x)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\bigvee_\\sigma v_\\sigma(x):= \\max_{\\sigma\\in\\Sigma} v_\\sigma(x)\n",
    "$$\n",
    "\n",
    "\n",
    "**Definition (Sublattice)**\n",
    "\n",
    "Let $X$ be a finite set. A subset $V$ of $\\mathbb{R}^X$ is called a sublattice of $\\mathbb{R}^X$ if \n",
    "\n",
    "$$\n",
    "u,v \\in V \\implies u\\wedge v\\in V, u\\vee v \\in V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43149fd8-b806-4146-909d-be7b6ebdfdae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55b627d7-f263-41a9-be18-543f0c9c7640",
   "metadata": {},
   "source": [
    "## Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587c5de-fc23-4ee9-8c0c-54acfc11434c",
   "metadata": {},
   "source": [
    "**Theorem**\n",
    "The value function $v^*$ is the solution of the Bellman equation.\n",
    "\n",
    "**Neumann Series Lemma**\n",
    "Let $\\rho(A)$ be the spectral radius of matrix A. If $\\rho(A)<1$, then\n",
    "\n",
    "- $I-A$ is nonsingular\n",
    "- $\\sum_{k\\ge 0} A^k$ converges to $(I-A)^{-1}$\n",
    "\n",
    " <font color=purple> How to see these results easily? also the following lemma. Grelfand also difficult to see. </font>\n",
    "\n",
    " <font color=purple> Should be able to have a dynamic version in the brain to see these </font>\n",
    "\n",
    "**Lemma**\n",
    "For any square matrix $B$ and any matrix norm $\\|\\cdot\\|$, we have\n",
    "\n",
    "- $\\rho(B)^k \\le \\|B^k\\|$, $\\forall k\\in\\mathbb{N}$\n",
    "\n",
    "- $\\rho(B) = \\lim_{k\\to\\infty} \\|B^k\\|^{1/k}$  (Gelfand's Formula)\n",
    "\n",
    "**Lemma**\n",
    "If $\\exists \\overline u \\in U, m\\in\\mathbb{N}$, s.t. $T^k u =\\overline u$, $\\forall u\\in U$ and $k\\ge m$, then $\\overline u$ is the unique fixed point of $T$ in $U$.\n",
    "\n",
    " <font color=purple> It stops at $\\overline u$ after \"more than\" $m$ times and you take $m+1$.  </font>\n",
    "\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "If\n",
    "\n",
    "- $T$ is globally stable on $U\\subset \\mathbb{R}^n$ with fixed point $u^*$ and\n",
    "- $C$ is nonempty, closed and invariant for $T$\n",
    "\n",
    "then, $u^*\\in C$\n",
    "\n",
    "\n",
    "**Banach's Contraction Mapping Theorem**\n",
    "\n",
    "If \n",
    "\n",
    "- $U$ is closed in $\\mathbb{R}^n$\n",
    "- $T$ is a contraction on $U$ wrt $\\|\\cdot\\|$\n",
    "\n",
    "then, there exists a unique fixed point $u^*\\in U$ such that\n",
    "\n",
    "$$\n",
    "\\|T^k-u^*\\|\\le\\lambda^k\\|u-u^*\\|\\,\\,\\forall k\\in\\mathbb{N}, u\\in U\n",
    "$$\n",
    "\n",
    "In particular, $T$ is globally stable in $U$.\n",
    "\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "If $f$ and $g$ are elements of $\\mathbb{R}^X$, then\n",
    "\n",
    "$$\n",
    "|\\max_{x\\in X}f(x)-\\max_{x\\in X}g(x)|\\le \\max_{x\\in X}|f(x)-g(x)|\n",
    "$$\n",
    "\n",
    "\n",
    "**Proposition**\n",
    "\n",
    "Let \n",
    "\n",
    "- $V$ be a sublattice of $\\mathbb{R}^X$\n",
    "- $\\{T_\\sigma\\}_{\\sigma\\in\\Sigma}$ be a finite family of self-mapping on $V$\n",
    "\n",
    "Set \n",
    "\n",
    "$$\n",
    "Tv = \\bigvee_{\\sigma\\in\\Sigma}T_\\sigma v \\,\\,\\,(v\\in V)\n",
    "$$\n",
    "By the sublattice property,  $T$ is a self-map on $V$.\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "If $T_\\sigma$ is a contraction of modulus $\\lambda_\\sigma$ w.r.t. $\\|\\cdot\\|_{\\infty}$ for each $\\sigma\\in\\Sigma$, then $T$ is a contraction of modulus $\\max_\\sigma \\lambda_\\sigma$ under the same norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57da38-e889-4fc6-b3de-d8cafe497747",
   "metadata": {},
   "source": [
    "## Structure of a typical dynamic program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472c8bd-82c3-4cad-a150-1bbc2d950a5b",
   "metadata": {},
   "source": [
    "For a time period $t<T$, we (objective is to maximize the expected lifetime rewards **EPV**)\n",
    "\n",
    "- observe the current state $X_t$\n",
    "\n",
    "- choose an action $A_t$\n",
    "\n",
    "- receive a reward $R_t(X_t,A_t)$\n",
    "\n",
    "- update $X_{t+1} = F(X_t, A_t, \\xi_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b86ca2-c968-4f00-b619-17fbbe67af1d",
   "metadata": {},
   "source": [
    "## Finite-Horizon Job Search Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54b7b2-4ae9-4731-8252-6ecb289a2c93",
   "metadata": {},
   "source": [
    "This section use a finite-horizon job search problem to illustrate finite-period DP. In particular\n",
    "\n",
    "- agent begins unemployed at time $t=0$\n",
    "\n",
    "- receive a new job offer paying wage $W_t$ for all $t = 0,1, 2,\\ldots, T$\n",
    "\n",
    "- Two choices and corresponding rewards\n",
    "  - *accept* $\\implies$ work *permanetly* with wage at the time accepting the offer\n",
    "  - *reject* $\\implies$ receive constant unemployment compensation $c$ for the current period\n",
    "  \n",
    "The state variable is wage $W_t\\sim \\varphi\\in \\mathcal{D}(W)\\,\\,iid, W\\subset \\mathbb{R}_{+}$ finite and $\\varphi$ is known. Action is to accept or reject the offer $A_{t}=0$ reject, $A_{t} = 1 $ accept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198c704-275d-4fc8-9d06-74ffbe666736",
   "metadata": {},
   "source": [
    "We can represent this problem using Bellman Equations for each period $t = 0,1,2,\\ldots, T$,i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_t(w_t) &= \\max\\left\\{\\text{stopping value, continuation value}\\right\\}\\\\\n",
    "&=\\max\\left\\{\\sum_{\\tau = 0}^{T-t} \\beta^\\tau w_t, c + \\beta \\sum_{w'\\in W} v_{t+1}(w')\\varphi(w')\\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can solve for all $v_t$ by backward induction to calculate the reservation wage at each period. This solves the problem of whether to accept or reject the offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99153798-08d1-4684-8080-fea2330b16da",
   "metadata": {},
   "source": [
    "### Code of Finite-Horizon Job Search Problem (T period)\n",
    "\n",
    "First, we start with importing `numpy` for numerical operations and `namedtuple` to store the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4667d245-a66e-4467-a28a-feb34fe46845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e8bdd-261f-4fb1-8408-27daae1a533e",
   "metadata": {},
   "source": [
    "A `namedtuple` is a convenient way to define a class. This type of data structure allows us to create tuple-like objects that have fields accessible by attribute lookup as well as being indexable and interable. \n",
    "\n",
    "In this model, we want to use the `namedtuple` to store values of the model, hence, we name it as `Model`. It requires the following parameters:\n",
    "\n",
    "- `c`: the unemployment compensation\n",
    "- `w_vals`: $W$, the finite wage space\n",
    "- `n`: the cardinality of the wage space (in the following code, I use the uniform distribution, hence, this simplies the answer by not including extra parameters for the pdf)\n",
    "- `β`: the discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3a883e-5c3b-4304-93c5-2ae96d6bac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = namedtuple(\"Model\", (\"c\", \"w_vals\", \"n\", \"β\",\"T\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ab2f2-1a89-4701-8313-6cc83af4dcc5",
   "metadata": {},
   "source": [
    "Then we use a function to input specific values into the `namedtuple`, i.e.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdd34e2-069c-446a-a963-a99d041c1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_search_model(\n",
    "    n = 50,          # wage grid size\n",
    "    w_min = 11,      # lowest wage\n",
    "    w_max = 60,      # highest wage\n",
    "    c = 10,          # unemployment compensation\n",
    "    β = 0.96,        # discount factor\n",
    "    T = 10           # number of periods t= 0, 1,...,T\n",
    "):\n",
    "    \"\"\"\n",
    "    This function input the paramters with the above default values, and return a namedtuple\n",
    "    \"\"\"\n",
    "    w_vals = np.linspace(w_min,w_max, n) # create a evenly spaced numbers over the specified intervals, with specified number of sample\n",
    "    \n",
    "    return Model(c = c, w_vals = w_vals, n = n, β = β, T=T) # return the namedtuple with the input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36819bf-993d-4e5a-8437-f69f19d6d883",
   "metadata": {},
   "source": [
    "Now we define a function that iteratively obtain the continuation value, and reservation wages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38a3ce5-9a36-4c3d-8eaa-c00f38c8db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservation_wage(model):\n",
    "    c, w_vals, n, β, T = model.c, model.w_vals, model.n, model.β, model.T  # Input the model parameters\n",
    "    H = np.zeros(T+1)  # Initialize the continuation value sequence\n",
    "    R = np.zeros(T+1)  # Initialize the reservation wage sequence\n",
    "    S = np.zeros((T+1,n))  # Initialize the maximum values at each state at each period\n",
    "    H[T] = c         # Input the last continuation value which is just the unemployment compensation\n",
    "    R[T] = c         # The reservation wage at the last period is just the unemployment compensation\n",
    "    S[T,:] = np.maximum(c, w_vals) # At period T, it is just comparing the unemployment compensation with the wages\n",
    "    for t in range(1, T+1):\n",
    "        H[T-t] = c + β * np.mean(S[T-t+1,:]) # Assuming uniform distribution, we only need to calculate the mean\n",
    "        df = np.geomspace(1, β**t, t+1)   # this generate the sequence for the denominator\n",
    "        dfs = np.sum(df)  # this is the denominator for the reservation wage calculation\n",
    "        R[T-t] = H[T+1-t]/dfs    # This calculate the reservation wage at time T-t\n",
    "        S[T-t,:] = np.maximum(dfs * w_vals, H[T-t])   # This returns the maximum values for each wage state by comparing the continuation value and stopping value\n",
    "    return R\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b1a1f-ed98-473d-8837-e71990d3bc28",
   "metadata": {},
   "source": [
    "This function iteratively generate the reservation wage sequence. We can show the result by create the model and use this function to calculate the reservation wage sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7839269-03b2-45e3-b295-61d46b9af3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36.50032766, 35.38365172, 34.07207926, 32.50704612, 30.6067209 ,\n",
       "       28.23780776, 25.19318638, 21.10849802, 15.29705719,  5.10204082,\n",
       "       10.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_job_search_model()\n",
    "reservation_wage(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e13c47-443d-4d1f-838f-85f3eeb85be9",
   "metadata": {},
   "source": [
    "The key idea is to break down this multi-stage decision problem into a two-stage decision problem. We obtain the value functions by comparing the continuation value and the stopping value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca13fb-f5fe-4577-9500-f6319fbcc2e2",
   "metadata": {},
   "source": [
    "## Infinite-Horizon Job Search Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e802c-e7ab-43d8-9c4b-56deaeb72d9b",
   "metadata": {},
   "source": [
    "The above example motivates the infinite-horizon job search problem. We let,\n",
    "\n",
    "- $v^*(w)$ denote the maximum lifetime EPV for the wage offer $w$.\n",
    "\n",
    "In the infinite horizon, we have\n",
    "\n",
    "$$\\text{Stopping value} = \\frac{w}{1-\\beta}$$\n",
    "\n",
    "$$\\text{Continuation value: }h^* = c+\\beta\\sum_{w'\\in W}v^*(w')\\varphi(w')$$\\\n",
    "This implies the optimal choice is\n",
    "$$\\mathbb{1}\\{\\text{Stopping value}\\ge \\text{Continuation value}\\} = \\mathbb{1}\\left\\{\\frac{w}{1-\\beta}\\ge h^*\\right\\}$$\n",
    "\n",
    "**Key Idea**\n",
    "Solve the Bellman equation to obtain the value function $v^*$, the corresponding Bellman equation is\n",
    "\n",
    "$$\n",
    "v^*(w) = \\max\\left\\{\\dfrac{w}{1-\\beta}, c+\\beta\\sum_{w'\\in W}v^*(w')\\varphi(w')\\right\\} \\,\\,\\,\\,(w\\in W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d778c3d-b0e3-48df-b264-550eeb487049",
   "metadata": {},
   "source": [
    "### Solve the Value function $v^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121b8f2-fcf6-4ec2-80a8-74e37c21cdb5",
   "metadata": {},
   "source": [
    "We first introduce the **Bellman operator**, defined at $v\\in\\mathbb{R}^W$ by\n",
    "\n",
    "$$\n",
    "(Tv)(w) = \\max\\left\\{\\frac{w}{1-\\beta}, c+\\beta\\sum_{w'\\in W} v(w')\\varphi(w')\\right\\} \\,\\,\\,(w\\in W)\n",
    "$$\n",
    "\n",
    "**Proposition**\n",
    "\n",
    "$T$ is a contraction on $R^W$ with respect to $\\|\\cdot\\|_{\\infty}$.\n",
    "\n",
    "This implies,\n",
    "\n",
    "- there exists a unique fixed point $\\overline v \\in\\mathbb{R}^W$ of $T$\n",
    "- $T^kv\\to \\overline v$ as $k\\to\\infty$ for all $v\\in \\mathbb{R}^W$.\n",
    "- $\\overline v = v^*$\n",
    "\n",
    "**Summary**\n",
    "\n",
    "We can compute $v^*$ by successive approximation:\n",
    "1. Choose any initial $v\\in\\mathbb{R}^W$\n",
    "2. Iterate with $T$ to obtain $T^kv\\approx v^*$ for some large $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59a8b5-fe68-4bec-8551-a8b79f5d09ad",
   "metadata": {},
   "source": [
    "### Optimal policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885933c-7625-49bb-9245-56ef08877f9f",
   "metadata": {},
   "source": [
    "In general, for a dynamic program, choices means a sequence of actions $(A_t)_{t\\ge 0}$, this specifies how agent will act at each $t$.\n",
    "\n",
    "We assume $A_t$ depends on current and past events, i.e.,\n",
    "\n",
    "$$\n",
    "A_t = \\sigma_t(X_t, A_{t-1}, X_{t-1}, A_{t-2},\\ldots, A_0,X_0)\n",
    "$$\n",
    "\n",
    "In DP, $\\sigma_t$ is called **policy function**.\n",
    "\n",
    "\n",
    "**Key idea** Design the state variable $X_t$ such that \n",
    "\n",
    "- it is sufficient to determine the optimal current action\n",
    "- but not so large as to be unmanagable\n",
    "\n",
    "\n",
    "**Job search problem**:\n",
    "\n",
    "- state variable is the current wage offer\n",
    "- actions are to accept or reject the current wage offer, i.e., $A_t = \\sigma(w)$\n",
    "- a policy $\\sigma: W\\mapsto \\{0,1\\}$\n",
    "\n",
    "Let $\\Sigma$ be the set of all such maps.\n",
    "\n",
    "**Definition ($v$-greedy policy)**\n",
    "\n",
    "for each $v\\in\\mathbb{R}^W$, a $v$-greedy policy implies $\\sigma \\in \\Sigma$ satisfying\n",
    "\n",
    "$$\n",
    "\\sigma(w)=\\mathbb{1}\\left\\{\\frac{w}{1-\\beta}\\ge c+\\beta\\sum_{w'\\in W} v(w')\\varphi(w')\\right\\} \n",
    "$$\n",
    "\n",
    "for all $w\\in W$.\n",
    "\n",
    "In this case, $\\sigma$ is a $v$-greedy policy $\\iff$ we accept the offer when the stopping value is greater than or equal to the continuation value **computed using $v$ **.\n",
    "\n",
    "\n",
    "**Optimal choice/policy**\n",
    "\n",
    "- agent adopt the $v^*$-greedy policy (**Bellman's principle of optimality**)\n",
    "\n",
    "$$\n",
    "\\sigma^*(w) = \\mathbb{1}\\{w\\ge w^*\\}\n",
    "$$\n",
    "where $w^*:= (1-\\beta) h^*$ is the reservation wage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85589ec-f122-4fa4-a5c9-7353b43bed31",
   "metadata": {},
   "source": [
    "#### Computation of Optimal policy using value function iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17384e2-ec1c-473c-87e3-0a78f23659fa",
   "metadata": {},
   "source": [
    "Since $T$ is globally stable on $\\mathbb{R}^W$, we can compute an approximate optimal policy by **value function iteration**,i.e., \n",
    "\n",
    "1. applying successive approximation on $T$ to compute $v^*$\n",
    "2. calculate $v^*-$greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487ff41-0676-4b51-8df0-1527054c13e1",
   "metadata": {},
   "source": [
    "## Value function iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285df4fc-70e0-4052-b0fc-0bc987a39b5c",
   "metadata": {},
   "source": [
    "We first create the function for successive approximate.\n",
    "\n",
    "Choose an arbitraty starting point $u_0$ and apply $S$ iteratively to get the fixed point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ac5f49-b646-4215-bcbe-6dd63e125607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def successive_approx (\n",
    "    S,                     # A callable operator\n",
    "    x_0,                   # Initial condition, the arbitary starting point\n",
    "    model,                 # Model parameters\n",
    "    tol = 1e-6,            # Error tolerance used for approximation\n",
    "    max_iter = 10_000,     # max interation to avoid infinite iteration due to not converging\n",
    "    print_step = 25        # Print at multiples of print_step\n",
    "):\n",
    "    x = x_0                # set the initial condition\n",
    "    error = tol + 1        # Initialize the error\n",
    "    k = 1                  # initialize the interations\n",
    "    \n",
    "    while (error > tol) and (k <= max_iter): \n",
    "        x_new = S(x,model)       # update by applying operator T\n",
    "        error = np.max(np.abs(x_new-x))  # the valuation of error is based on the norm\n",
    "        if k % print_step == 0:  # the remainder of k/print_step is zero\n",
    "            print(f\"Completed iteration {k} with error {error}.\") # This prints the error value if the steps is divisible by the print_step\n",
    "        x = x_new         # assign the new value to x\n",
    "        k += 1            # update the number of step by 1\n",
    "    if error <= tol:      # After the iteration finishes and if error is small\n",
    "        print(f\"Terminated successfully in {k} interations.\")\n",
    "    else:     \n",
    "        print(\"Warning: hit iteration bound.\")\n",
    "    return x              # if successful, x is the fixed point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6248b-bf86-4ede-af16-68c385975600",
   "metadata": {},
   "source": [
    "Now we define the Bellman operator as discussed before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265358a9-891b-4f52-94c4-9fbc8311a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def S (\n",
    "    v,        # the value function\n",
    "    model     # model parameters\n",
    "):\n",
    "    c, w_vals, n, β, T = model.c, model.w_vals, model.n, model.β, model.T  # Input the model parameters\n",
    "    return np.maximum(w_vals/(1-β), c + β * np.mean(v))                    # return the value as state before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9665973-9eb6-4474-bc05-ef288ef558fb",
   "metadata": {},
   "source": [
    "Then, we construct a function to obtain the $v-$greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fbcee2-2b4d-4228-b91e-dbbba386d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy (\n",
    "    v,        # the value function, a np array\n",
    "    model     # model parameters\n",
    "): \n",
    "    c, w_vals, n, β, T = model.c, model.w_vals, model.n, model.β, model.T  # Input the model parameters\n",
    "    σ= np.where(w_vals/(1-β) >= c + β * np.mean(v), 1, 0)                  # v-greedy\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bceffe1-937e-4ed2-9661-0d2e3f4c6ad3",
   "metadata": {},
   "source": [
    "Now we use value function iteration method to obtain the value function $v^*$ and optimal policy/$v^*$-greedy policy $\\sigma^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53117bfb-08cc-444e-9e00-3208aafd3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function_iteration (model):\n",
    "    c, w_vals, n, β, T = model.c, model.w_vals, model.n, model.β, model.T  # Input the model parameters\n",
    "    v_init = np.zeros(n) # initialize the guess of value function\n",
    "    v_star = successive_approx(S, v_init, model)\n",
    "    σ_star = get_greedy(v_star, model)\n",
    "    \n",
    "    return v_star, σ_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed61c3c1-dc35-48e0-8c82-413dfdc378b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 25 with error 0.03455934557837281.\n",
      "Completed iteration 50 with error 6.7009955273533706e-06.\n",
      "Terminated successfully in 57 interations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1200.        , 1225.        , 1250.        ,\n",
       "        1275.        , 1300.        , 1325.        , 1350.        ,\n",
       "        1375.        , 1400.        , 1425.        , 1450.        ,\n",
       "        1475.        , 1500.        ]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function_iteration(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31543709-7320-4513-91b6-2a584a4926e1",
   "metadata": {},
   "source": [
    "## Method II: Compute the continuation value directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a80de-1dd0-456e-9747-8d1cef483bfb",
   "metadata": {},
   "source": [
    "Idea: compute the continuation value $h^*$ directly. This shifts the problem from $n$-dimension to one dimension. \n",
    "\n",
    "**Method**:\n",
    "\n",
    "Recall, we have\n",
    "\n",
    "$$\n",
    "v^*(w) = \\max\\left\\{\\frac{w}{1-\\beta}, c + \\beta \\sum_{w'\\in W} v^*(w')\\varphi(w')\\right\\}\n",
    "$$\n",
    "\n",
    "where $h^* = c + \\beta \\sum_{w'\\in W} v^*(w')\\varphi(w')$ this implies, \n",
    "\n",
    "$$\n",
    "v^*(w') = \\max\\left\\{\\frac{w'}{1-\\beta}, h^*\\right\\}\n",
    "$$\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "h^* = c+\\beta \\sum_{w'\\in W}v^*(w')\\varphi(w') =c+\\beta \\sum_{w'\\in W}\\max\\left\\{\\frac{w'}{1-\\beta}, h^*\\right\\}\\varphi(w')\n",
    "$$\n",
    "\n",
    "In a similar fashion, we introduce a mapping $g:\\mathbb{R}_+\\mapsto\\mathbb{R}_+$,\n",
    "\n",
    "$$\n",
    "g(h) = c+\\beta \\sum_{w'\\in W}\\max\\left\\{\\frac{w'}{1-\\beta}, h\\right\\}\\varphi(w')\n",
    "$$\n",
    "\n",
    "We can show that $g$ is a contraction on $\\mathbb{R}_+$.\n",
    "\n",
    "\n",
    "**New Algorithm**\n",
    "\n",
    "1. compute $h^*$ via successive approximation on $g$ (iterate in $\\mathbb{R}$ not in $\\mathbb{R}^n$)\n",
    "\n",
    "2. Optimal policy is \n",
    "\n",
    "$$\n",
    "\\sigma^*(w) = \\mathbb{1}\\left\\{\\frac{w}{1-\\beta}\\ge h^*\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f62a8188-21c6-4469-8ce7-13005a885718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g (\n",
    "    h,      #input reservation wage\n",
    "    model   # Model parameters\n",
    "):\n",
    "    c, w_vals, n, β, T = model.c, model.w_vals, model.n, model.β, model.T  # Input the model parameters\n",
    "    return c + β * np.mean(np.maximum(w_vals/(1-β), h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed38dd5a-b9cc-45de-bbd2-ad31f07f9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_iteration (model):\n",
    "    c, w_vals, n, β, T = model.c, model.w_vals, model.n, model.β, model.T  # Input the model parameters\n",
    "    h_init = 0    # inital guess\n",
    "    h_star = successive_approx(g, h_init, model)  # succesive approximation on g\n",
    "    v_star = np.maximum(w_vals/(1-β),h_star)\n",
    "    return v_star, h_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652cdb7f-cb4e-4888-b34b-843bd45d158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 25 with error 0.024550959099087777.\n",
      "Completed iteration 50 with error 4.7603871280443855e-06.\n",
      "Terminated successfully in 56 interations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1198.06629623, 1198.06629623, 1198.06629623,\n",
       "        1198.06629623, 1200.        , 1225.        , 1250.        ,\n",
       "        1275.        , 1300.        , 1325.        , 1350.        ,\n",
       "        1375.        , 1400.        , 1425.        , 1450.        ,\n",
       "        1475.        , 1500.        ]),\n",
       " 1198.0662962297258)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direct_iteration(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5239c-2d7d-4d58-bb0c-3a6d34e1cf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
