{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474cfd65-adb7-47b8-96b8-a731f425ef11",
   "metadata": {},
   "source": [
    "# Chapter 5 Markov Decision Process Reading Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08035e-c0cb-4a9c-b9c3-2df54824d531",
   "metadata": {},
   "source": [
    "- discrete time, infinite horizon, dynamic program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d0013-ebdd-454b-842b-ce9c47c54d0a",
   "metadata": {},
   "source": [
    "1. Definitions and Properties:\n",
    "    - Model specification\n",
    "    - Examples: Renewal, optimal inventory, cake eating, optimal stopping\n",
    "    - Optimality theory\n",
    "    - Algorithm: VFI,HPI,HPI VS NI, OPI\n",
    "2. Detailed applications\n",
    "    - Optimal Inventories\n",
    "    - Optimal saving and labor income: MDP representation, Implementation, timing, output\n",
    "    - Optimal Investment: description, MDP representation, Implementation\n",
    "    \n",
    "3. Modified Bellman equation:\n",
    "    - Structure estimation: definition, illustration, expected value function, optimality vs EV\n",
    "    - Gumbel Max Trick\n",
    "    - Optimal savings with stochastic return on wealth\n",
    "    - Q-Factors\n",
    "    - Operator factorization: Refactoring the Bellman operator, refactorization and optimality, refactored OPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce4cab9-31e9-49a6-aa03-ed4b507dc939",
   "metadata": {},
   "source": [
    "## Theorems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50fde7-84e2-446d-a6ad-8f2958ba5686",
   "metadata": {},
   "source": [
    "### Proposition 5.1.1. Optimality of MDP\n",
    "\n",
    "If $\\mathcal{M}=(\\Gamma, \\beta, r, P)$ is a MDP with the value function $v^*$ and Bellman operator $T$, then\n",
    "\n",
    "1. $v^*$ is the unique solution to the Bellman equation\n",
    "2. $\\lim_{k\\to\\infty} T^k v=v^*$ for all $v\\in\\mathbb{R}^X$ (Global stability)\n",
    "3. Bellman's principle of optimality holds,\n",
    "4. at least one optimal policy exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb73c8f-ff76-40c2-83f9-df75bd661629",
   "metadata": {},
   "source": [
    "### Lemma 5.1.2. $\\beta P_\\sigma$ is subgradient of $T$ at v, $\\sigma$ is v-greedy\n",
    "\n",
    "If $v\\in\\mathbb{R}^X$ and $\\sigma\\in\\Sigma$ is $v$-greedy, the $\\beta P_\\sigma$ is a subgradient of $T$ at $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b6ba1-e9a5-459f-a82e-92ba00ec084c",
   "metadata": {},
   "source": [
    "## MDP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc62c0e-2b95-44cd-b109-39576e61f513",
   "metadata": {},
   "source": [
    "Now actions have affects on rewards, next state, also transition probabilities. Compared to Optimal stopping (binary choice). Now the feasible actions depends on state and are not restricted to binary choices.\n",
    "\n",
    "A controller who interacts with a state process $(X_t)_{t\\ge 0}$ by choosing an action path $(A_t)_{t\\ge 0}$ to maximize expected discounted rewards,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\sum_{t\\ge 0}\\beta^t r(X_t,A_t)\n",
    "$$\n",
    "\n",
    "taking the initial state $X_0$ as given.\n",
    "\n",
    "**Controller is clairvoyant: cannot choose actions that depends on future states**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334db12-4170-4c63-b343-d6f83cc6e3a4",
   "metadata": {},
   "source": [
    "We take \n",
    "\n",
    "- $X$: finite state space\n",
    "- $A$: finite action space\n",
    "- $\\Gamma: X\\mapsto \\mathscr{P}(A)$: a non-emptycorrespondence from $X$ to the subsets of $A$, i.e., $\\Gamma(x)\\neq \\emptyset\\,\\,\\forall x\\in X$\n",
    "\n",
    "**We define the Markov Decision Process as a 4-Tuple $\\mathcal{M}=(\\Gamma, \\beta, r, P)$**\n",
    "\n",
    "1. **Feasible correpondence** $\\Gamma: X\\mapsto \\mathscr{P}(A)$, induces the **feasible state-action pairs**\n",
    "\n",
    "$$\n",
    "G:=\\{(x,a)\\in X\\times A: x\\in X, a\\in \\Gamma(x)\\}\n",
    "$$\n",
    "\n",
    "2. **Discount factor**: $\\beta\\in(0,1)$\n",
    "3. **Reward function**: $r\\in\\mathbb{R}^G$, $r:G\\mapsto \\mathbb{R}$\n",
    "4. **Stochastic kernel**: $P:G\\times X \\mapsto \\mathbb{R}_+$ satisfying:\n",
    "\n",
    "$$\n",
    "\\sum_{x'\\in X}P(x,a,x')=1,\\,\\,\\forall (x,a)\\in G\n",
    "$$\n",
    "\n",
    "**Bellman Equation** corresponding to $\\mathcal{M}$:\n",
    "\n",
    "$$\n",
    "v(x) = \\max_{a\\in \\Gamma(x)}\\left\\{r(x,a)+\\beta\\sum_{x'}v(x')P(x,a,x')\\right\\} \\,\\,\\forall x\\in X\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408efd1-d683-46e1-8004-80c1a7ac2bdb",
   "metadata": {},
   "source": [
    "We can understand the Bellman equation as reducing an infinite-horizon problem to a two period problem involving the present and the future. \n",
    "\n",
    "Current actions influence:\n",
    "\n",
    "1. Current reward\n",
    "2. Expected discounted value from future states\n",
    "\n",
    "In every ccase, **there is a tradeoff between maximizing current rewards and shifting probability mass towards states with high future rewards**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b5be8-486a-48da-8038-1ffd127f82bf",
   "metadata": {},
   "source": [
    "# Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59624a86-427e-4393-b78e-b3071c972d84",
   "metadata": {},
   "source": [
    "## Policies and Lifetime values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669cc40-0a2b-462d-b226-671877a98bf7",
   "metadata": {},
   "source": [
    "Let $\\mathcal{M} = (\\Gamma, \\beta,r,P)$ be a MDP.\n",
    "\n",
    "**Feasible policy set**\n",
    "\n",
    "$$\n",
    "\\Sigma:=\\{\\sigma\\in A^X: \\sigma(x)\\in \\Gamma(x)\\,\\,\\forall x\\in X\\}\n",
    "$$\n",
    "\n",
    "**If we select a policy $\\sigma\\in\\Sigma$, it is understood that we respond to the state $X_t$ with action $A_t =\\sigma(X_t)$ for all $t$**.\n",
    "\n",
    "Hence, the state evolving by drawing $X_{t+1}$ from the stochastic kernel under $\\sigma$ becomes,\n",
    "\n",
    "$$\n",
    "P(X_t,\\sigma(X_t),\\cdot)\n",
    "$$\n",
    "\n",
    "We denote $(X_t)_{t\\ge 0}$ as a $P_\\sigma$-Markov when,\n",
    "\n",
    "$$\n",
    "P_\\sigma(x,x') = P(x,\\sigma(x),x')\n",
    "$$\n",
    "\n",
    "(Left hand side is $P_\\sigma$-Markov, RHS is a stochastic kernel)\n",
    "\n",
    "**Note $P_\\sigma\\in\\mathscr{M}(\\mathbb{R}^X)$. Hence, fixing a policy closes the loop in the state transition process and defines a Markov chain for the state.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f884a68-6f59-4dc1-916b-c4553005eed3",
   "metadata": {},
   "source": [
    "Under the policy $\\sigma$, rewards at state $x$ are $r(x,\\sigma(x))$.\n",
    "\n",
    "We denote $r_\\sigma(x) = r(x,\\sigma(x))$ and $\\mathbb{E}_x = \\mathbb{E}[\\cdot|X_0=x]$.\n",
    "\n",
    "Then the **lifetime value of following $\\sigma$ from initial state $x$ can be written as**\n",
    "\n",
    "$$\n",
    "v_\\sigma(x) =\\mathbb{E}_x \\sum_{t\\ge 0}\\beta^t r_\\sigma(X_t), \\,\\,\\,\\text{$(X_t)$ is a $P_\\sigma$ Markov with $X_0=x$}\n",
    "$$\n",
    "\n",
    "If $\\beta<1$ we have,\n",
    "\n",
    "**$\\sigma$-value function**\n",
    "$$\n",
    "v_\\sigma =\\sum_{t\\ge 0}\\beta^t P_\\sigma^t r_\\sigma = (I-\\beta P_\\sigma)^{-1} r_\\sigma\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f28b7-ea4c-46f1-9cb7-56af2120f29f",
   "metadata": {},
   "source": [
    "**Policy Operator Correspond to $\\sigma$**\n",
    "\n",
    "(since we already specify $\\sigma$, no need for max)\n",
    "\n",
    "$$\n",
    "(T_\\sigma v)(x) = r_\\sigma(x)+\\beta\\sum_{x'\\in X} v(x')P_\\sigma(x,x')\n",
    "$$\n",
    "\n",
    "In vector form, we have,\n",
    "\n",
    "$$\n",
    "T_\\sigma v = r_\\sigma + \\beta P_\\sigma v\n",
    "$$\n",
    "\n",
    "Computationally, we can iterating with $T_\\sigma$ can obtain an approximation of $v_\\sigma$.\n",
    "\n",
    "- For small state spaces, we can use $v_\\sigma = (I-\\beta P_\\sigma)^{-1} r_\\sigma$\n",
    "- If $X$ is very large, inverting the matrix $(I-\\beta P_\\sigma)$ is computationally demanding. \n",
    "- In such setting, iterating $T_\\sigma$ is better.\n",
    "\n",
    "**Interpretation of Policy operator**\n",
    "\n",
    "$(T^k_\\sigma v)(x)$ is the payoff from following $\\sigma$ and starting in state $x$ when lifetime is truncated to the finite horizon $k$ and $v$ provides a terminal payoff in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70ba72-b9aa-473e-8682-47aeda5de84d",
   "metadata": {},
   "source": [
    "## Defining Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f147a10-e20e-47e1-a2a7-a57b6c5db1b6",
   "metadata": {},
   "source": [
    "Given MDP $\\mathcal{M} = (\\Gamma,\\beta, r, P)$ with $\\sigma$-value function $\\{v_\\sigma\\}_{\\sigma\\in\\Sigma}$.\n",
    "\n",
    "The **value function** corresponding to $\\mathcal{M}$ is defined as \n",
    "\n",
    "$$\n",
    "v^* = \\bigvee_{\\sigma\\in\\Sigma} v_\\sigma, \\,\\,\\,v^*(x) = \\max_{\\sigma\\in\\Sigma} v_\\sigma(x)\n",
    "$$\n",
    "\n",
    "Interpretation: the maximal lifetime value we can extract from feasible behaviors. The maximum exist at each $x$ because $X, A$, hence $\\Sigma$ is finite.\n",
    "\n",
    "A policy $\\sigma\\in\\Sigma$ is called **optimal policy** for $\\mathcal{M}$ if $v_\\sigma =v^*$. In other words, a policy is optimal if its lifetime value is maximal at each state.\n",
    "\n",
    "**v-greedy**\n",
    "\n",
    "Given $v\\in\\mathbb{R}^X$, we define a policy $\\sigma\\in\\Sigma$ to be **v-greedy** if\n",
    "\n",
    "$$\n",
    "\\sigma(x)\\in\\arg\\max_{a\\in\\Gamma(x)} \\left\\{r(x,a)+\\beta\\sum_{x'} v(x')P(x,a,x')\\right\\}\n",
    "$$\n",
    "\n",
    "**In essence, a $v$-greedy policy treats $v$ as the correct value function and sets all actions accordingly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571feb6f-f74a-4c77-bb96-3f8fc536771c",
   "metadata": {},
   "source": [
    "**Bellman's principle of optimality**\n",
    "\n",
    "Bellman's principle of optimality is said to hold for the MDP $\\mathcal{M}$ if\n",
    "\n",
    "$$\n",
    "\\sigma\\in\\Sigma \\,\\,\\text{is optimal for $\\mathcal{M}\\iff$$\\sigma$ is $v^*$-greedy}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c66880-2936-4c7b-ae96-f3fd596e21e5",
   "metadata": {},
   "source": [
    "**Bellman operator**\n",
    "\n",
    "The Bellman operator corresponding to $\\mathcal{M}$ is the self-map $T$ on $\\mathbb{R}^X$ defined by \n",
    "\n",
    "$$\n",
    "(Tv)(x)=\\max_{a\\in\\Gamma(x)}\\left\\{r(x,a)+ \\beta\\sum_{x'}v(x')P(x,a,x')\\right\\}\n",
    "$$\n",
    "\n",
    "**The Bellman operator is the pointwise maximum of $\\{T_\\sigma\\}_{\\sigma\\in\\Sigma}$**, hence, we can express the Bellman operator as\n",
    "\n",
    "$$\n",
    "T = \\bigvee_{\\sigma\\in\\Sigma} T_\\sigma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c996b36-a908-4374-80f5-5e482f8398c6",
   "metadata": {},
   "source": [
    "## Optimality Theory\n",
    "\n",
    "It is important to understand the importance of Bellman's principle of optimality. \n",
    "\n",
    "Greedy policies are relatively easy to compute.\n",
    "\n",
    "Solving \n",
    "\n",
    "$$\n",
    "\\sigma(x) =\\arg\\max_{a\\in\\Gamma(x)} \\left\\{r(x,a)+\\beta\\sum_{x'\\in X} v(x')P(x,a,x')\\right\\}\n",
    "$$\n",
    "\n",
    "at each $x$ is easier than trying to directly solve the problem of maximizing lifetime value, i.e,\n",
    "\n",
    "$$\n",
    "v^*(x) = \\max_{\\sigma\\in\\Sigma} v_\\sigma(x)\n",
    "$$\n",
    "\n",
    "This is because $|\\Sigma|\\gg |\\Gamma(x)|$. \n",
    "\n",
    "The Bellman's principle of optimality tells us that solving the overall problem reduces to computing a $v$-greedy policy with the right choice of $v$.\n",
    "\n",
    "For optimal stopping problems, that choice is the value function $v^*$.\n",
    "\n",
    "Intuitively, $v^*$ assigns a correct value to each state, in the sense of maximal lifetime value the controller can extract, so using $v^*$ to calculate greedy policies leads to the optimal outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23739108-a509-48e3-9478-f49c009237dc",
   "metadata": {},
   "source": [
    "## Examples of MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac676117-abc4-4c99-85b0-4adeb338cbb3",
   "metadata": {},
   "source": [
    "### Renewal Problem: Rust(1987), engine replacement problem for bus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b77b0-1cdd-4413-8a03-3e103c995c62",
   "metadata": {},
   "source": [
    "In each period, the superintendent decides whether or not to replace the engine of a given bus. Replacement is costly but delaying risks unexpected failure. \n",
    "\n",
    "Consider an abstract version of Rust's problem with **binary action** $A_t=\\{0,1\\}$, \n",
    "\n",
    "- when $A_t=1$, the state resets to some fixed **renewal state** $\\bar x \\in X$.\n",
    "\n",
    "- when $A_t=0$, the state updates according to $Q\\in \\mathscr{M}(\\mathbb{R}^X)$.\n",
    "\n",
    "Given current state $x$ and action $a$, current reward $r(x,a)$ is received. The discount rate is $\\beta$.\n",
    "\n",
    "**Bellman equation**\n",
    "\n",
    "$$\n",
    "v(x)= \\max\\{r(x,1)+ \\beta v(\\bar x), r(x,0)+\\beta\\sum_{x'}v(x')Q(x,x')\\}\n",
    "$$\n",
    "\n",
    "**MDP representation**\n",
    "\n",
    "To set the problem up as an MDP we set \n",
    "\n",
    "- $A=\\{0,1\\}$\n",
    "- $\\Gamma(x) = A$ for all $x\\in A$\n",
    "\n",
    "We define the stochastic kernel as\n",
    "\n",
    "$$\n",
    "P(x,a,x') = a\\mathbb{1}\\{x'=\\bar x\\}+(1-a)Q(x,x')\\tag{$(x,a)\\in G, x'\\in X$}\n",
    "$$\n",
    "\n",
    "Inserting this $P$, we get the MDP form of Bellman equation:\n",
    "\n",
    "$$\n",
    "v(x)=\\max_{a\\in\\{0,1\\}}\\left\\{r(x,a)+ a\\beta v(\\bar x) + (1-a)\\beta\\sum_{x'\\in X}v(x')Q(x,x')\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d2f72-ff34-419f-b5bb-a095b91753a9",
   "metadata": {},
   "source": [
    "### Optimal inventory management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795acce-9156-4348-908c-58658c44ef89",
   "metadata": {},
   "source": [
    "A firm where a manager maximizes shareholder value. To simplify, we ignore exit option, hence the value of the firm is the EPV of future profits.\n",
    "\n",
    "Assume the firm only sell one product with profit function $\\pi_t$, and let $r>0$ be the interest rate.\n",
    "\n",
    "**value of the firm** is\n",
    "\n",
    "$$\n",
    "V_0 = \\mathbb{E}\\sum_{t\\ge 0} \\beta^t \\pi_t\n",
    "$$\n",
    "\n",
    "The firm faces **exogenous demand process**\n",
    "\n",
    "$$\n",
    "(D_t)\\sim_{IID}\\varphi\\in\\mathcal{D}(\\mathbb{Z}_+)\n",
    "$$\n",
    "\n",
    "The **Inventory** $(X_t)_{t\\ge 0}$ obeys the law of motion:\n",
    "\n",
    "$$\n",
    "X_{t+1} = f(X_t,D_t,A_t) = \\max\\{X_t-D_{t+1}, 0\\}+A_t\n",
    "$$\n",
    "\n",
    "The **Action $A_t$ is the unit of stock ordered this period, which take one period to arrive.**\n",
    "\n",
    "\n",
    "We assume that firms cannot sell more stock than they have at hand and can store at maximum $K$ items. We set the price equals to 1. \n",
    "\n",
    "**Profit function**\n",
    "\n",
    "$$\n",
    "\\pi_{t} = X_t\\wedge D_{t+1}-cA_t - \\kappa\\mathbb{1}\\{A_t>0\\}\n",
    "$$\n",
    "\n",
    "- $c$ is the per unit cost, $\\kappa$ is the fixed cost related to order inventory\n",
    "- $X_t\\wedge D_{t+1}$, implies firm cannot sell more than they have at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e492233-0e44-4aed-b588-11d2f1229e58",
   "metadata": {},
   "source": [
    "### MDP formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333be17f-f391-4b76-8e94-51ba9aad99dd",
   "metadata": {},
   "source": [
    "Let $X$ be the **state space** of inventory at storage, i.e.,\n",
    "\n",
    "$$\n",
    "X = \\{0,1,2,\\cdots,K\\}\n",
    "$$\n",
    "\n",
    "Then the **feasible correspondence** is,\n",
    "\n",
    "$$\n",
    "\\Gamma(x) = \\{0,1,2,\\cdots,K-x\\}\n",
    "$$\n",
    "\n",
    "**Reward function is the current expected profit**\n",
    "\n",
    "$$\n",
    "r(x,a) = \\left(\\sum_{d\\ge 0} (x\\wedge d)\\varphi(d)\\right)-ca-\\kappa\\mathbb{1}\\{a>0\\}\n",
    "$$\n",
    "\n",
    "**Stochastic kernel**\n",
    "\n",
    "$$\n",
    "P(x,a,x') = \\mathbb{P}\\{f(x,D,a)=x'\\}, \\tag{$D\\sim \\varphi$}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd477c4a-de58-4be6-9fac-8be54c3e611a",
   "metadata": {},
   "source": [
    "**Bellman Equation**\n",
    "\n",
    "$$\n",
    "v(x)= \\max_{a\\in\\Gamma(x)}\\left\\{r(x,a)+\\beta\\sum_{x'\\in X} v(x')P(x,a,x')\\right\\}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "v(x) =\\max_{a\\in\\Gamma(x)}\\left\\{r(x,a)+\\beta\\sum_{d\\ge 0} v(f(x,a,d))\\varphi(d)\\right\\}\n",
    "$$\n",
    "\n",
    "**Bellman operator**\n",
    "\n",
    "$$\n",
    "(Tv)(x) = \\max_{a\\in \\Gamma(x)}\\left\\{r(x,a)+\\beta\\sum_{d\\ge 0}v(f(x,a,d))\\varphi(d)\\right\\}\n",
    "$$\n",
    "\n",
    "The operator maps $\\mathbb{R}^X$ to itself is designed so that its set of fixed points coincide with the solution of the Bellman equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f095358-1056-4c6a-b8c7-e3f017f09dd7",
   "metadata": {},
   "source": [
    "### Cake Eating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b580c6-3164-460f-adce-79c7d0130181",
   "metadata": {},
   "source": [
    "A household with finite wealth endowment but has no labor income. The wealth evolves as\n",
    "\n",
    "$$\n",
    "W_{t+1} = R(W_t-C_t)\n",
    "$$\n",
    "\n",
    "where $C_t$ is current consumption, $R$ is the gross interest rate. Household has utilty function $u(C_t)$ and aim to maximize lifetime utility\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\sum_{t\\ge 0}\\beta^t u(C_t), W_0=w\n",
    "$$\n",
    "\n",
    "by choosing the consumption path or similarly by choosing the next period wealth level $w'=R(w-c)\\implies c = w-w'/R$\n",
    "\n",
    "**Bellman equation**\n",
    "\n",
    "$$\n",
    "v(w) = \\max_{0\\le w'\\le w} \\{u(w-w'/R)+\\beta v(w')\\}\n",
    "$$\n",
    "\n",
    "Hence, household uses the Bellman equation to trade-off current utility with future value.\n",
    "\n",
    "**MDP formulation**\n",
    "\n",
    "A MDP is a 4-tuple $\\mathcal{M}=(\\Gamma,\\beta, r, P)$.\n",
    "\n",
    "With $W$ the wealth space as the state space. We can first formulate the feasible correspondence $\\Gamma(w)$.\n",
    "\n",
    "Since houshold can only consume less or equal to that what they have and also greater than 0, this implies, with $W_0=w$, we have\n",
    "\n",
    "$$\n",
    "\\Gamma(w) = \\{0,1,\\cdots, w\\},\\tag{$w\\in W$}\n",
    "$$\n",
    "\n",
    "Then, we have discount factor $\\beta = 1/(1+r)$ where $r=R-1$. Hence, $\\beta =1/R$.\n",
    "\n",
    "The reward function is the utility function, we have,\n",
    "\n",
    "$$\n",
    "u(c)= u(w,w') = u(w-w'/R)\n",
    "$$\n",
    "\n",
    "Since there is no uncertainty in this case, there is no stochastic kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afb13c-2843-4398-871b-fc31cfc56c53",
   "metadata": {},
   "source": [
    "### Optimal Stopping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26b488-fee2-4d7e-a586-322bd97aabf5",
   "metadata": {},
   "source": [
    "We can frame optimal stopping problem as MDP by adding new state variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0860a-9c00-4104-8bdf-973899e9bb7f",
   "metadata": {},
   "source": [
    "Consider a job search model with Markov wage, i.e., $(W_t)_{t\\ge 0}$ is a $Q$-Markov on finite $W$.\n",
    "\n",
    "To express the job search problem as MDP, we let\n",
    "\n",
    "$$\n",
    "X = \\{0,1\\}\\times W\n",
    "$$\n",
    "\n",
    "be a state space whose typical element is $(e,w)$ with $e$ representing the employment status.\n",
    "\n",
    "The action space $A=\\{0,1\\}$ denoting rejecting or accepting the wage offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448501a-860c-4d8c-bf9e-c9e2af121452",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7ea92-996f-4560-b3cb-e71ef2ba8ffd",
   "metadata": {},
   "source": [
    "## Value function iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0f39b-e672-4565-868e-063db5dd8373",
   "metadata": {},
   "source": [
    "Value function iteration for MDPs is similar to VFI for the job search model.\n",
    "\n",
    "We use successive approximation on $T$ to compute an approximation $v_k$ to the value function $v^*$ and then take a $v_k$-greedy policy.\n",
    "\n",
    "- Easy to understand and implement\n",
    "- less efficient compared to HPI and OPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bf9d7-fc7b-4f47-892b-f955161b9fe7",
   "metadata": {},
   "source": [
    "## Howard Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658f817-f81a-4a3f-9d06-01b0d9a58760",
   "metadata": {},
   "source": [
    "Howard Policy Iteration computes optimal policies by iterating between computing the value of a given policy and computing the greedy policy associated with that value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4286e-dfa8-4844-8b7e-f6c405b04644",
   "metadata": {},
   "source": [
    "1. Given initial $\\sigma\\in\\Sigma$ as the inital policy\n",
    "2. Obtain the $\\sigma$-value function $v_\\sigma$\n",
    "3. Get $v_\\sigma$-greedy policy, $\\sigma'$     (Policy Improvement)\n",
    "4. Get $\\sigma'$-value function, $v_\\sigma$'   (Policy Evaluation)\n",
    "\n",
    "$\\vdots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28866ea3-c0b9-4ca4-b2d9-f1a754dad3bf",
   "metadata": {},
   "source": [
    "ATTRACTIVE:\n",
    "\n",
    "1. In finite state setting, the algorithm always converges to an exact optimal policy in a finite number of steps, regardless of the initial condition.\n",
    "\n",
    "2. The rate of convergence is faster than VFI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea508d-fade-4e28-9fa4-745d54209202",
   "metadata": {},
   "source": [
    "### HPI as Newton Iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21b7fe-4f4e-4fdf-80b0-50ed97c07575",
   "metadata": {},
   "source": [
    "Since $T$ is not always differentiable, we can substitute derivatives using **subgradient**\n",
    "\n",
    "\n",
    "Given a self-map $T:S\\mapsto S$, $S\\subset \\mathbb{R}^n$. An $n\\times n$ matrix $D$ is called **subgradient** of $T$ at $v\\in S$ if\n",
    "\n",
    "$$\n",
    "Tu \\ge Tv + D(u-v) \\,\\,\\forall u\\in S\n",
    "$$\n",
    "\n",
    "- If $T$ convex and differentiable, there is only 1 subgradient equals to the derivative\n",
    "\n",
    "- If $T$ convex but non-differentiable, then there may exist multiple subgradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52102ea0-e58d-4f5f-8116-598bd1fa8d8a",
   "metadata": {},
   "source": [
    "Lemma 5.1.2.: For $v\\in\\mathbb{R}^X$, $\\sigma$ is v-greedy, then $\\beta P_\\sigma$ is the subgradient of $T$ at $v$.\n",
    "\n",
    "We use this, from Newton's iteration using subdifferntial, we can find this leads us to iterate on\n",
    "\n",
    "$$\n",
    "v_{k+1} = Qv_k\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "Qv = (I-\\beta P_\\sigma)^{-1} (Tv - \\beta P_\\sigma v)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is $v$-greedy policy.\n",
    "\n",
    "Since $\\sigma$ is $v$-greedy, this implie $Tv=T_\\sigma v=r_\\sigma + \\beta P_\\sigma v$.\n",
    "\n",
    "Hence, we have,\n",
    "\n",
    "$$\n",
    "Qv = (I-\\beta P_\\sigma)^{-1} r_\\sigma\n",
    "$$\n",
    "\n",
    "this is the HPI policy evaluation to update the $\\sigma$-value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657ec77-bb55-43e3-af81-2d2bb133cbc9",
   "metadata": {},
   "source": [
    "The fact that HPI is a version of Newton's method suggests that its iterates $(v_k)_{k\\ge 0}$ enjoys quadratic convergence. \n",
    "\n",
    "Under mild conditions, one can show there exists a constant $N$ such that for all $k\\ge 0$,\n",
    "\n",
    "$$\n",
    "\\|v_{k+1}-v_k\\|\\le N\\|v_k-v_{k-1}\\|^2\n",
    "$$\n",
    "\n",
    "Hence, HPI enjoys both a fast convergence rate and the robustness of global convergence.\n",
    "\n",
    "However, HPI is not always optimal in terms of efficiency, since the size of the constant term also matters. This term can be large because at each step, the update from $v_\\sigma$ to $v_{\\sigma'}$ **requires computing the exact lifetime value** $v_{\\sigma'}$ of the $v_\\sigma$-greedy policy $\\sigma'$.\n",
    "\n",
    "Computing this fixed point exactly can be computationally expensive in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecdd141-1fee-4a34-b3d0-74bf9f95f199",
   "metadata": {},
   "source": [
    "## Optimistic Policy Iteration (OPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98b71e-7e7c-4291-8bec-d50b66c22be2",
   "metadata": {},
   "source": [
    "Optimistic Policy Iteration borrows from VFI and HPI.\n",
    "\n",
    "The algorithm is the same as HPI but instead of computing the full value $v_\\sigma$ of a given policy, the approximation $T^m_\\sigma v$ is used.\n",
    "\n",
    "\n",
    "In the algorithm, the policy operator $T_{\\sigma_k}$ is applied $m$ times to generate an approximation of $v_{\\sigma_k}$.\n",
    "\n",
    "Then constant step size $m$ can also be replaced with a sequence $(m_k)\\subset \\mathbb{N}$. \n",
    "\n",
    "In either case, for MDPs, convergence to an optimal policy is guaranteed.\n",
    "\n",
    "**Convex Combination of VFI and HPI**\n",
    "\n",
    "As $m\\to\\infty$, the algorithm increasingly approximates Howard Policy Iteratio, since $T^m_{\\sigma_k}$\n",
    "\n",
    "At the same time, as $m=1$, OPI reduces to VFI. \n",
    "\n",
    "In almost all dynamic programming application, there exists choices of $m>1$ such that OPI converges faster than VFI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1899f-3f85-4305-8492-98c2ac2471bf",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b929a4-4b1a-46f4-83b2-46d4e0fb4acd",
   "metadata": {},
   "source": [
    "## Optimal Inventories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941914f-1710-4b0a-9918-2d5f53730995",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
