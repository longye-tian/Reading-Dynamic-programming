{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474cfd65-adb7-47b8-96b8-a731f425ef11",
   "metadata": {},
   "source": [
    "# Chapter 5 Markov Decision Process Reading Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08035e-c0cb-4a9c-b9c3-2df54824d531",
   "metadata": {},
   "source": [
    "- discrete time, infinite horizon, dynamic program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d0013-ebdd-454b-842b-ce9c47c54d0a",
   "metadata": {},
   "source": [
    "1. Definitions and Properties:\n",
    "    - Model specification\n",
    "    - Examples: Renewal, optimal inventory, cake eating, optimal stopping\n",
    "    - Optimality theory\n",
    "    - Algorithm: VFI,HPI,HPI VS NI, OPI\n",
    "2. Detailed applications\n",
    "    - Optimal Inventories\n",
    "    - Optimal saving and labor income: MDP representation, Implementation, timing, output\n",
    "    - Optimal Investment: description, MDP representation, Implementation\n",
    "    \n",
    "3. Modified Bellman equation:\n",
    "    - Structure estimation: definition, illustration, expected value function, optimality vs EV\n",
    "    - Gumbel Max Trick\n",
    "    - Optimal savings with stochastic return on wealth\n",
    "    - Q-Factors\n",
    "    - Operator factorization: Refactoring the Bellman operator, refactorization and optimality, refactored OPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b6ba1-e9a5-459f-a82e-92ba00ec084c",
   "metadata": {},
   "source": [
    "## MDP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc62c0e-2b95-44cd-b109-39576e61f513",
   "metadata": {},
   "source": [
    "Now actions have affects on rewards, next state, also transition probabilities. Compared to Optimal stopping (binary choice). Now the feasible actions depends on state and are not restricted to binary choices.\n",
    "\n",
    "A controller who interacts with a state process $(X_t)_{t\\ge 0}$ by choosing an action path $(A_t)_{t\\ge 0}$ to maximize expected discounted rewards,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\sum_{t\\ge 0}\\beta^t r(X_t,A_t)\n",
    "$$\n",
    "\n",
    "taking the initial state $X_0$ as given.\n",
    "\n",
    "**Controller is clairvoyant: cannot choose actions that depends on future states**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334db12-4170-4c63-b343-d6f83cc6e3a4",
   "metadata": {},
   "source": [
    "We take \n",
    "\n",
    "- $X$: finite state space\n",
    "- $A$: finite action space\n",
    "- $\\Gamma: X\\mapsto \\mathscr{P}(A)$: a non-emptycorrespondence from $X$ to the subsets of $A$, i.e., $\\Gamma(x)\\neq \\emptyset\\,\\,\\forall x\\in X$\n",
    "\n",
    "**We define the Markov Decision Process as a 4-Tuple $\\mathcal{M}=(\\Gamma, \\beta, r, P)$**\n",
    "\n",
    "1. **Feasible correpondence** $\\Gamma: X\\mapsto \\mathscr{P}(A)$, induces the **feasible state-action pairs**\n",
    "\n",
    "$$\n",
    "G:=\\{(x,a)\\in X\\times A: x\\in X, a\\in \\Gamma(x)\\}\n",
    "$$\n",
    "\n",
    "2. **Discount factor**: $\\beta\\in(0,1)$\n",
    "3. **Reward function**: $r\\in\\mathbb{R}^G$, $r:G\\mapsto \\mathbb{R}$\n",
    "4. **Stochastic kernel**: $P:G\\times X \\mapsto \\mathbb{R}_+$ satisfying:\n",
    "\n",
    "$$\n",
    "\\sum_{x'\\in X}P(x,a,x')=1,\\,\\,\\forall (x,a)\\in G\n",
    "$$\n",
    "\n",
    "**Bellman Equation** corresponding to $\\mathcal{M}$:\n",
    "\n",
    "$$\n",
    "v(x) = \\max_{a\\in \\Gamma(x)}\\left\\{r(x,a)+\\beta\\sum_{x'}v(x')P(x,a,x')\\right\\} \\,\\,\\forall x\\in X\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408efd1-d683-46e1-8004-80c1a7ac2bdb",
   "metadata": {},
   "source": [
    "We can understand the Bellman equation as reducing an infinite-horizon problem to a two period problem involving the present and the future. \n",
    "\n",
    "Current actions influence:\n",
    "\n",
    "1. Current reward\n",
    "2. Expected discounted value from future states\n",
    "\n",
    "In every ccase, **there is a tradeoff between maximizing current rewards and shifting probability mass towards states with high future rewards**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b5be8-486a-48da-8038-1ffd127f82bf",
   "metadata": {},
   "source": [
    "# Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59624a86-427e-4393-b78e-b3071c972d84",
   "metadata": {},
   "source": [
    "## Policies and Lifetime values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669cc40-0a2b-462d-b226-671877a98bf7",
   "metadata": {},
   "source": [
    "Let $\\mathcal{M} = (\\Gamma, \\beta,r,P)$ be a MDP.\n",
    "\n",
    "**Feasible policy set**\n",
    "\n",
    "$$\n",
    "\\Sigma:=\\{\\sigma\\in A^X: \\sigma(x)\\in \\Gamma(x)\\,\\,\\forall x\\in X\\}\n",
    "$$\n",
    "\n",
    "**If we select a policy $\\sigma\\in\\Sigma$, it is understood that we respond to the state $X_t$ with action $A_t =\\sigma(X_t)$ for all $t$**.\n",
    "\n",
    "Hence, the state evolving by drawing $X_{t+1}$ from the stochastic kernel under $\\sigma$ becomes,\n",
    "\n",
    "$$\n",
    "P(X_t,\\sigma(X_t),\\cdot)\n",
    "$$\n",
    "\n",
    "We denote $(X_t)_{t\\ge 0}$ as a $P_\\sigma$-Markov when,\n",
    "\n",
    "$$\n",
    "P_\\sigma(x,x') = P(x,\\sigma(x),x')\n",
    "$$\n",
    "\n",
    "(Left hand side is $P_\\sigma$-Markov, RHS is a stochastic kernel)\n",
    "\n",
    "**Note $P_\\sigma\\in\\mathscr{M}(\\mathbb{R}^X)$. Hence, fixing a policy closes the loop in the state transition process and defines a Markov chain for the state.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f884a68-6f59-4dc1-916b-c4553005eed3",
   "metadata": {},
   "source": [
    "Under the policy $\\sigma$, rewards at state $x$ are $r(x,\\sigma(x))$.\n",
    "\n",
    "We denote $r_\\sigma(x) = r(x,\\sigma(x))$ and $\\mathbb{E}_x = \\mathbb{E}[\\cdot|X_0=x]$.\n",
    "\n",
    "Then the **lifetime value of following $\\sigma$ from initial state $x$ can be written as**\n",
    "\n",
    "$$\n",
    "v_\\sigma(x) =\\mathbb{E}_x \\sum_{t\\ge 0}\\beta^t r_\\sigma(X_t), \\,\\,\\,\\text{$(X_t)$ is a $P_\\sigma$ Markov with $X_0=x$}\n",
    "$$\n",
    "\n",
    "If $\\beta<1$ we have,\n",
    "\n",
    "**$\\sigma$-value function**\n",
    "$$\n",
    "v_\\sigma =\\sum_{t\\ge 0}\\beta^t P_\\sigma^t r_\\sigma = (I-\\beta P_\\sigma)^{-1} r_\\sigma\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23739108-a509-48e3-9478-f49c009237dc",
   "metadata": {},
   "source": [
    "## Examples of MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac676117-abc4-4c99-85b0-4adeb338cbb3",
   "metadata": {},
   "source": [
    "### Renewal Problem: Rust(1987), engine replacement problem for bus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b77b0-1cdd-4413-8a03-3e103c995c62",
   "metadata": {},
   "source": [
    "In each period, the superintendent decides whether or not to replace the engine of a given bus. Replacement is costly but delaying risks unexpected failure. \n",
    "\n",
    "Consider an abstract version of Rust's problem with **binary action** $A_t=\\{0,1\\}$, \n",
    "\n",
    "- when $A_t=1$, the state resets to some fixed **renewal state** $\\bar x \\in X$.\n",
    "\n",
    "- when $A_t=0$, the state updates according to $Q\\in \\mathscr{M}(\\mathbb{R}^X)$.\n",
    "\n",
    "Given current state $x$ and action $a$, current reward $r(x,a)$ is received. The discount rate is $\\beta$.\n",
    "\n",
    "**Bellman equation**\n",
    "\n",
    "$$\n",
    "v(x)= \\max\\{r(x,1)+ \\beta v(\\bar x), r(x,0)+\\beta\\sum_{x'}v(x')Q(x,x')\\}\n",
    "$$\n",
    "\n",
    "**MDP representation**\n",
    "\n",
    "To set the problem up as an MDP we set \n",
    "\n",
    "- $A=\\{0,1\\}$\n",
    "- $\\Gamma(x) = A$ for all $x\\in A$\n",
    "\n",
    "We define the stochastic kernel as\n",
    "\n",
    "$$\n",
    "P(x,a,x') = a\\mathbb{1}\\{x'=\\bar x\\}+(1-a)Q(x,x')\\tag{$(x,a)\\in G, x'\\in X$}\n",
    "$$\n",
    "\n",
    "Inserting this $P$, we get the MDP form of Bellman equation:\n",
    "\n",
    "$$\n",
    "v(x)=\\max_{a\\in\\{0,1\\}}\\left\\{r(x,a)+ a\\beta v(\\bar x) + (1-a)\\beta\\sum_{x'\\in X}v(x')Q(x,x')\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d2f72-ff34-419f-b5bb-a095b91753a9",
   "metadata": {},
   "source": [
    "### Optimal inventory management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795acce-9156-4348-908c-58658c44ef89",
   "metadata": {},
   "source": [
    "A firm where a manager maximizes shareholder value. To simplify, we ignore exit option, hence the value of the firm is the EPV of future profits.\n",
    "\n",
    "Assume the firm only sell one product with profit function $\\pi_t$, and let $r>0$ be the interest rate.\n",
    "\n",
    "**value of the firm** is\n",
    "\n",
    "$$\n",
    "V_0 = \\mathbb{E}\\sum_{t\\ge 0} \\beta^t \\pi_t\n",
    "$$\n",
    "\n",
    "The firm faces **exogenous demand process**\n",
    "\n",
    "$$\n",
    "(D_t)\\sim_{IID}\\varphi\\in\\mathcal{D}(\\mathbb{Z}_+)\n",
    "$$\n",
    "\n",
    "The **Inventory** $(X_t)_{t\\ge 0}$ obeys the law of motion:\n",
    "\n",
    "$$\n",
    "X_{t+1} = f(X_t,D_t,A_t) = \\max\\{X_t-D_{t+1}, 0\\}+A_t\n",
    "$$\n",
    "\n",
    "The **Action $A_t$ is the unit of stock ordered this period, which take one period to arrive.**\n",
    "\n",
    "\n",
    "We assume that firms cannot sell more stock than they have at hand and can store at maximum $K$ items. We set the price equals to 1. \n",
    "\n",
    "**Profit function**\n",
    "\n",
    "$$\n",
    "\\pi_{t} = X_t\\wedge D_{t+1}-cA_t - \\kappa\\mathbb{1}\\{A_t>0\\}\n",
    "$$\n",
    "\n",
    "- $c$ is the per unit cost, $\\kappa$ is the fixed cost related to order inventory\n",
    "- $X_t\\wedge D_{t+1}$, implies firm cannot sell more than they have at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e492233-0e44-4aed-b588-11d2f1229e58",
   "metadata": {},
   "source": [
    "### MDP formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333be17f-f391-4b76-8e94-51ba9aad99dd",
   "metadata": {},
   "source": [
    "Let $X$ be the **state space** of inventory at storage, i.e.,\n",
    "\n",
    "$$\n",
    "X = \\{0,1,2,\\cdots,K\\}\n",
    "$$\n",
    "\n",
    "Then the **feasible correspondence** is,\n",
    "\n",
    "$$\n",
    "\\Gamma(x) = \\{0,1,2,\\cdots,K-x\\}\n",
    "$$\n",
    "\n",
    "**Reward function is the current expected profit**\n",
    "\n",
    "$$\n",
    "r(x,a) = \\left(\\sum_{d\\ge 0} (x\\wedge d)\\varphi(d)\\right)-ca-\\kappa\\mathbb{1}\\{a>0\\}\n",
    "$$\n",
    "\n",
    "**Stochastic kernel**\n",
    "\n",
    "$$\n",
    "P(x,a,x') = \\mathbb{P}\\{f(x,D,a)=x'\\}, \\tag{$D\\sim \\varphi$}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd477c4a-de58-4be6-9fac-8be54c3e611a",
   "metadata": {},
   "source": [
    "**Bellman Equation**\n",
    "\n",
    "$$\n",
    "v(x)= \\max_{a\\in\\Gamma(x)}\\left\\{r(x,a)+\\beta\\sum_{x'\\in X} v(x')P(x,a,x')\\right\\}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "v(x) =\\max_{a\\in\\Gamma(x)}\\left\\{r(x,a)+\\beta\\sum_{d\\ge 0} v(f(x,a,d))\\varphi(d)\\right\\}\n",
    "$$\n",
    "\n",
    "**Bellman operator**\n",
    "\n",
    "$$\n",
    "(Tv)(x) = \\max_{a\\in \\Gamma(x)}\\left\\{r(x,a)+\\beta\\sum_{d\\ge 0}v(f(x,a,d))\\varphi(d)\\right\\}\n",
    "$$\n",
    "\n",
    "The operator maps $\\mathbb{R}^X$ to itself is designed so that its set of fixed points coincide with the solution of the Bellman equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f095358-1056-4c6a-b8c7-e3f017f09dd7",
   "metadata": {},
   "source": [
    "### Cake Eating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b580c6-3164-460f-adce-79c7d0130181",
   "metadata": {},
   "source": [
    "A household with finite wealth endowment but has no labor income. The wealth evolves as\n",
    "\n",
    "$$\n",
    "W_{t+1} = R(W_t-C_t)\n",
    "$$\n",
    "\n",
    "where $C_t$ is current consumption, $R$ is the gross interest rate. Household has utilty function $u(C_t)$ and aim to maximize lifetime utility\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\sum_{t\\ge 0}\\beta^t u(C_t), W_0=w\n",
    "$$\n",
    "\n",
    "by choosing the consumption path or similarly by choosing the next period wealth level $w'=R(w-c)\\implies c = w-w'/R$\n",
    "\n",
    "**Bellman equation**\n",
    "\n",
    "$$\n",
    "v(w) = \\max_{0\\le w'\\le w} \\{u(w-w'/R)+\\beta v(w')\\}\n",
    "$$\n",
    "\n",
    "Hence, household uses the Bellman equation to trade-off current utility with future value.\n",
    "\n",
    "**MDP formulation**\n",
    "\n",
    "A MDP is a 4-tuple $\\mathcal{M}=(\\Gamma,\\beta, r, P)$.\n",
    "\n",
    "With $W$ the wealth space as the state space. We can first formulate the feasible correspondence $\\Gamma(w)$.\n",
    "\n",
    "Since houshold can only consume less or equal to that what they have and also greater than 0, this implies, with $W_0=w$, we have\n",
    "\n",
    "$$\n",
    "\\Gamma(w) = \\{0,1,\\cdots, w\\},\\tag{$w\\in W$}\n",
    "$$\n",
    "\n",
    "Then, we have discount factor $\\beta = 1/(1+r)$ where $r=R-1$. Hence, $\\beta =1/R$.\n",
    "\n",
    "The reward function is the utility function, we have,\n",
    "\n",
    "$$\n",
    "u(c)= u(w,w') = u(w-w'/R)\n",
    "$$\n",
    "\n",
    "Since there is no uncertainty in this case, there is no stochastic kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afb13c-2843-4398-871b-fc31cfc56c53",
   "metadata": {},
   "source": [
    "### Optimal Stopping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26b488-fee2-4d7e-a586-322bd97aabf5",
   "metadata": {},
   "source": [
    "We can frame optimal stopping problem as MDP by adding new state variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0860a-9c00-4104-8bdf-973899e9bb7f",
   "metadata": {},
   "source": [
    "Consider a job search model with Markov wage, i.e., $(W_t)_{t\\ge 0}$ is a $Q$-Markov on finite $W$.\n",
    "\n",
    "To express the job search problem as MDP, we let\n",
    "\n",
    "$$\n",
    "X = \\{0,1\\}\\times W\n",
    "$$\n",
    "\n",
    "be a state space whose typical element is $(e,w)$ with $e$ representing the employment status.\n",
    "\n",
    "The action space $A=\\{0,1\\}$ denoting rejecting or accepting the wage offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448501a-860c-4d8c-bf9e-c9e2af121452",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
