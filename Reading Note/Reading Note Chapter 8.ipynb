{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8705be-3a0f-4307-bac0-a52f7f36f891",
   "metadata": {},
   "source": [
    "# Definitions and Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f1f6a-16d2-40a1-ac38-b9ccd52fc0a3",
   "metadata": {},
   "source": [
    "## Defining RDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1373c35-3bd2-45bd-9cab-e1a6f199a1d9",
   "metadata": {},
   "source": [
    "Consider a generic Bellman equation\n",
    "\n",
    "$$\n",
    "v(x) = \\max_{a\\in\\Gamma{x}}B(x,a,v)\n",
    "$$\n",
    "\n",
    "where, \n",
    "\n",
    "- $x$ is state\n",
    "- $a$ is action\n",
    "- $\\Gamma$ is a feasible correspondence\n",
    "- $B$ is an aggregator function\n",
    "\n",
    "\n",
    "**Recursive Decision Process**\n",
    "\n",
    "We define a **Recursive Decision Process (RDP)** to be a triple $\\mathcal{R}=(\\Gamma, V, B)$ consisting of\n",
    "\n",
    "- **Feasible correspondence** $\\Gamma$ that is a nonempty correpondence from $X$ to $A$, which in turns defines\n",
    "\n",
    "   - **Feasible state-action pair** $G=\\{(x,a)\\in X\\times A: a\\in \\Gamma(x)\\}$\n",
    "   - **Feasible policy set** $\\Sigma=\\{\\sigma \\in A^X: \\sigma(x)\\in \\Gamma(x)\\}$\n",
    "\n",
    "- **Value space** A subset $V\\subset\\mathbb{R}^X$\n",
    "- **Value aggregator** $B: G\\times V\\to \\mathbb{R}$, it is the total lifetime reward corresponding to current action $a$, current state $x$ and value function $v$, that satisfies\n",
    "   - **Monotonicity**: $v,w\\in V, v\\le w\\implies B(x,a,v)\\le B(x,a,w)$\n",
    "   - **Consistency**: $w(x) = B(x,\\sigma(x),w)$ for some $\\sigma\\in\\Sigma$ and $v\\in V\\implies w\\in V$.\n",
    " \n",
    "(The monotonicity condition states relative to $v$, if rewards are at least as high for $w$ in every future state, then the total rewards one can extract under $w$ should be at least as high.)\n",
    "\n",
    "(The consistency condition ensures that as we consider values of different policies we remain within the value space $V$).\n",
    "\n",
    "We can treat MDP as a special case of RDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1763aab-8e36-4018-8fd5-d8d961061513",
   "metadata": {},
   "source": [
    "## Lifetime Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b156e-747e-4a05-96e6-8829c7eafb8b",
   "metadata": {},
   "source": [
    "### Policy and Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3145d-3ad7-47c5-a133-18dfde4cff79",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R}=(\\Gamma, V,B)$ be an RDP with state and action space $X$ and $A$, let $\\Sigma$ be the set of all feasible policies. For each $\\sigma\\in\\Sigma$, we introduce the **policy operator** $T_\\sigma$ as a self-map on $V$ defined by\n",
    "\n",
    "$$\n",
    "(T_\\sigma v)(x) = B(x,\\sigma(x),x)\n",
    "$$\n",
    "\n",
    "and $T_\\sigma$ is an order-preserving self-map on $V$.\n",
    "\n",
    "If $T_\\sigma$ has a unique fixed point in $V$, we denote this fixed point by $v_\\sigma$ and call it the $\\sigma$-value function.\n",
    "\n",
    "We can interpret $v_\\sigma$ as representing the lifetime value of following policy $\\sigma$.\n",
    "\n",
    "\n",
    "**IN RDP**\n",
    "\n",
    "The policy operator can be expressed as $(T_\\sigma v)(x) = A_\\sigma(x,(R_\\sigma v)(x))$ for some aggregator $A_\\sigma$ and certainty equivalent operator $R_\\sigma$. \n",
    "\n",
    "Hence $T_\\sigma$ is a Koopmans operator and lifetime value associated with policy $\\sigma$ is the fixed point of this operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36860eaf-261f-47ae-a3fa-4e8eb35c6238",
   "metadata": {},
   "source": [
    "### Uniqueness and Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc9e19-ba39-46d4-bef0-a5f040a9f06c",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R} = (\\Gamma, V, B)$ be a given RDP with policy operators $\\{T_\\sigma\\}$\n",
    "\n",
    "Given that our objective is to maximize lifetime value over the set of policies in $\\Sigma$, we need to assume at the very least that lifetime value is well defined at each policy.\n",
    "\n",
    "**Well-Posed**\n",
    "\n",
    "We say that $\\mathcal{R}$ is **well-posed** if for all $\\sigma\\in\\Sigma$, $T_\\sigma$ has a unique fixed point $v_\\sigma\\in V$.\n",
    "\n",
    "**Globally stable**\n",
    "\n",
    "Let $\\mathcal{R}$ be an RDP with policy operators $\\{T_\\sigma\\}_{\\sigma\\in\\Sigma}$.\n",
    "\n",
    "We say $\\mathcal{R}$ is **globally stable** if for all $\\sigma\\in\\Sigma$, $T_\\sigma$ is globally stable on $V$.\n",
    "\n",
    "**Every gloablly stable RDP is well-posed.**\n",
    "\n",
    "**Global stability implies that for any choice of terminal condition, finite horizon valuations always converge to their infinite horizon counterparts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa9da5-fce1-4bb8-ac89-e4cad3431571",
   "metadata": {},
   "source": [
    "### Continuity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2e3b9-9a44-42d2-bcec-bc0d30743be5",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R}=(\\Gamma, V,B)$ be an RDP. \n",
    "\n",
    "We call $\\mathcal{R}$ **continuous** if $B(x,a,v)$ is continuous in $v$ for all $(x,a)\\in G$. \n",
    "\n",
    "In other words, $\\mathcal{R}$ is continuous if for any $v\\in V$, any $(x,a)\\in G$ and any sequence $(v_k)_{k\\ge 1}$ in $V$, we have,\n",
    "\n",
    "$$\n",
    "\\lim_{k\\to\\infty} v_k = v \\implies \\lim_{k\\to\\infty}B(x,a,v_k) = B(x,a,v) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656ed06-b351-4cec-a405-ae2b668b54df",
   "metadata": {},
   "source": [
    "## Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae32ab5-64e9-4fa5-8ce0-a2057108a8f0",
   "metadata": {},
   "source": [
    "### Greedy policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2097d3-180b-4478-8506-5ca776c1f3fe",
   "metadata": {},
   "source": [
    "Given an RDP $\\mathcal{R} = (\\Gamma, V,B)$ and $v\\in V$, a policy $\\sigma\\in\\Sigma$ is called $v$-greedy if\n",
    "\n",
    "$$\n",
    "\\sigma(x)\\in\\arg\\max_{a\\in\\Gamma(x)}B(x,a,v)\n",
    "$$\n",
    "\n",
    "for all $x\\in X$. \n",
    "\n",
    "**Since $\\Gamma(x)$ is finite and nonempty at each $x\\in X$, at least one such policy exists.**\n",
    "\n",
    "We say that $v\\in V$ satisfies the **Bellman equation** if\n",
    "\n",
    "$$\n",
    "v(x)=\\max_{a\\in\\Gamma(x)}B(x,a,v)\n",
    "$$\n",
    "\n",
    "for all $x\\in X$. The **Bellman operator** correponding to $\\mathcal{R}$ is the map $T$ on $V$ defined by\n",
    "\n",
    "$$\n",
    "(Tv)(x) = \\max_{a\\in\\Gamma(x)}B(x,a,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c6012-a486-46db-9ac2-0da29c7f6924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
